<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Observability in 2022: Why it matters and how OpenTelemetry can help</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help" /><author><name>Ben Evans</name></author><id>058dd256-91a7-4914-b9eb-7cc4fd8a2040</id><updated>2022-04-12T07:00:00Z</updated><published>2022-04-12T07:00:00Z</published><summary type="html">&lt;div class="paragraph"&gt; &lt;p&gt;This article explains the basics of &lt;em&gt;observability&lt;/em&gt; for developers. We&amp;#8217;ll look at why observability should interest you, its current level of maturity, and what to look out for to make the most of its potential.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Two years ago, James Governor of the developer analyst firm Redmonk remarked, "Observability is making the transition from being a niche concern to becoming a new frontier for user experience, systems, and service management in web companies and enterprises alike." Today, observability is hitting the mainstream.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As 2022 gets underway, you should expect to hear more about observability as a concern to take seriously. However, lots of developers are still unsure about what observability actually is&amp;#8212;&amp;#8203;and some of the descriptions of the subject can be vague and imprecise. Read on to get a foundation in this emerging topic.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_what_is_observability"&gt;What is observability?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The discipline of observability grew out of several separate strands of development, including application performance monitoring (APM) and the need to make orchestrated systems such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; more comprehensible. Observability aims to provide highly granular insights into the behavior of systems, along with rich context.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Overall, implementing observability is conceptually fairly simple. To enable observability in your projects, you should:&lt;/p&gt; &lt;/div&gt; &lt;div class="olist arabic"&gt; &lt;ol class="arabic"&gt; &lt;li&gt; &lt;p&gt;Instrument systems and applications to collect relevant data (e.g. metrics, traces, and logs).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Send this data to a separate external system that can store and analyze it.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide visualizations and insights into systems as a whole (including query capability for end users).&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The final step&amp;#8212;&amp;#8203;the query and visualization capabilities&amp;#8212;&amp;#8203;are key to the power of observability.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The theoretical background for the approach comes from system control theory&amp;#8212;&amp;#8203;essentially asking, "How well can the internal state of a system be inferred from outside?"&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This requirement has been set down as a constraint in a &lt;a href="https://www.honeycomb.io/blog/so-you-want-to-build-an-observability-tool/"&gt;blog posting by Charity Majors&lt;/a&gt;: "Observability requires that you not have to predefine the questions you will need to ask, or optimize those questions in advance." Meeting this constraint requires the collection of sufficient data to accurately model the system&amp;#8217;s internal state.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Incident resolution is a really good fit for observability, and it&amp;#8217;s where the practice originated. Site reliability experts (SREs) and &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; teams typically focus on incident response. They are interested in a holistic understanding of complex behavior that replaces fragmentary or pre-judged views based on just one or two pieces of the overall system.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;But if the right data is being collected, the stakeholders for observability are much broader than just SREs, production support, and DevOps folks. Observability gives rise to different goals depending on the stakeholder group using the data.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The promise of a wider use for observability can be seen in some of the discussions about whether observability systems should also collect business-relevant metrics, costs, etc. Such data provides additional context and possible use cases for an observability system, but some practitioners argue that it dilutes the goal of the system.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_why_is_observability_important"&gt;Why is observability important?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As applications increasingly move to the cloud, they are becoming more complex. There are typically more services and components in modern applications, with more complex topology as well as a much faster pace of change (driven by practices such as &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The growing complexity of applications parallels the increasing popularity of technologies with genuinely new behaviors that were created for the cloud. The highlights here include &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; environments, dynamically scaling services (especially Kubernetes), and Function-as-a-Service deployments such as AWS Lambda.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This new world makes root cause analysis and incident resolution potentially a lot harder, yet the same questions still need answering:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;What is the overall health of my solution?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What is the root cause of errors and defects?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What are the performance bottlenecks?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which of these problems could impact the user experience?&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability is therefore at the heart of architecting robust, reliable systems for the cloud. The search for an architectural solution to these questions is perhaps best expressed in a &lt;a href="https://copyconstruct.medium.com/monitoring-and-observability-8417d1952e1c"&gt;posting by Cindy Sridharan&lt;/a&gt;: "Since it&amp;#8217;s still not possible to predict every single failure mode a system could potentially run into or predict every possible way in which a system could misbehave, it becomes important that we build systems that can be debugged armed with evidence and not conjecture."&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This new cloud-native world is especially important to Red Hat users because so much of it is based on open source and open standards. The core runtimes, instrumentation components, and telemetry are all open source, and observability components are managed through industry bodies such as the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_what_data_do_we_need_to_collect"&gt;What data do we need to collect?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability data is often conceptualized in terms of three pillars:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Distributed traces&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Metrics and monitoring&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Logs&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Although some have questioned the value of this categorization, it is a relatively simple mental model, and so is quite useful for developers who are new to observability. Let&amp;#8217;s discuss each pillar in turn.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_distributed_traces"&gt;Distributed traces&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A &lt;em&gt;distributed trace&lt;/em&gt; is a record of a single service invocation, usually corresponding to a single request from an individual user. The trace includes the following metadata about each request:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Which instance was called&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which containers they were running on&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which method was invoked&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;How the request performed&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What the results were&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In distributed architectures, a single service invocation typically triggers downstream calls to other services. These calls, which contribute to the overall trace, are known as &lt;em&gt;spans&lt;/em&gt;, so a trace forms a tree structure of spans. Each span has associated metadata.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The span perspective corresponds to the &lt;em&gt;extrinsic&lt;/em&gt; view of service calls in traditional monitoring. Distributed traces are used to instrument service calls that are request-response oriented. There are additional difficulties associated with calls that do not fit this pattern that tracing struggles to account for.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_metrics_and_monitoring"&gt;Metrics and monitoring&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;em&gt;Metrics&lt;/em&gt; are numbers measuring specific activity over a time interval. A metric is typically encoded as a tuple consisting of a timestamp, name, value, and dimensions. The dimensions are a set of key-value pairs that describe additional metadata about the metric. Furthermore, it should be possible for a data storage engine to aggregate values across the dimensions of a particular metric to create a meaningful result.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There are many examples of metrics across many different aspects of a software system:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;System metrics (including CPU, memory, and disk usage)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Infrastructure metrics (e.g., from AWS CloudWatch)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Application metrics (such as APM or error tracking)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;User and web tracking scripts (e.g., from Google Analytics)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Business metrics (e.g., customer sign-ups)&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Metrics can be gathered from all of the different levels on which the application operates, from very low-level operating system counters all the way up to human and business-scale metrics. Unlike logs or traces, the data volume of metrics does not scale linearly with request traffic&amp;#8212;&amp;#8203;the exact relationship varies based on the type of metric being collected.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_logs"&gt;Logs&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;em&gt;Logs&lt;/em&gt; constitute the third pillar of observability. These are defined as immutable records of discrete events that happen over time. Depending on the implementation, there are basically three types of logs: plain text, structured, and binary format. Not all observability products support all three.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Examples of logs include:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;System and server logs (syslog)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Firewall and network system logs&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Application logs (e.g., Log4j)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Platform and server logs (e.g., Apache, NGINX, databases)&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_observability_tools_open_source_offerings_on_the_rise"&gt;Observability tools: Open source offerings on the rise&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The APM/monitoring market segment used to be dominated by proprietary vendors. In response, various &lt;a href="https://developers.redhat.com/topics/open-source"&gt;free and open source&lt;/a&gt; software projects started or were spun out of tech companies. Early examples include &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; for metrics, and &lt;a href="https://zipkin.io/"&gt;Zipkin&lt;/a&gt; and &lt;a href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt; for tracing. In the logging space, the "ELK stack" (&lt;a href="https://www.elastic.co/"&gt;Elasticsearch&lt;/a&gt;, &lt;a href="https://www.elastic.co/logstash/"&gt;Logstash&lt;/a&gt;, and &lt;a href="https://www.elastic.co/kibana/"&gt;Kibana&lt;/a&gt;) gained market share and became popular.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As software continues to become more complex, more and more resources are required to provide a credible set of instrumentation components. For proprietary observability products, this trend creates duplication and inefficiency. The market has hit an inflection point, and it is becoming more efficient for competing companies to collaborate on an open source core and compete on features further up the stack (as well as on pricing).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This historical pattern is not an unusual dynamic for open source, and has shown up as a switch from proprietary to open source driven offerings as this market segment migrates from APM to observability. The move to open source can also be partly attributed to the influence of observability startups that have been fully or partially open source since their inception.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;One key milestone was the merger of the OpenTracing and OpenCensus projects to form &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt;, a major project within CNCF. The project is still maturing, but is gaining momentum. An increasing number of users are investigating and implementing OpenTelemetry, and this number seems set to grow significantly during 2022. A recent survey from the CNCF showed that 49 percent of users were already using OpenTelemetry, and that number is rising rapidly.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_the_state_of_opentelemetry"&gt;The state of OpenTelemetry&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Some developers are still confused by exactly what OpenTelemetry actually is. The project offers a set of standards, formats, client libraries, and associated software components. The standards are explicitly cross-platform and not tied to any particular technology stack.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;OpenTelemetry provides a framework that integrates with open source and commercial products and can collect observability data from apps written in many languages. Because the implementations are open source, they are at varying levels of technical maturity, depending on the interest that OpenTelemetry has attracted in specific language communities.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;From the Red Hat perspective, the &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java/JVM&lt;/a&gt; implementation is particularly relevant, being one of the most mature implementations. Components in other major languages and frameworks, such as &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;, are also fairly mature.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The implementations work with applications running on bare metal or virtual machines, but OpenTelemetry overall is definitely a cloud-first technology.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;It&amp;#8217;s also important to recognize what OpenTelemetry is &lt;em&gt;not.&lt;/em&gt; It isn&amp;#8217;t a data ingest, storage, backend, or visualization component. Such components must be provided either by other open source projects or by vendors to produce a full observability solution.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_observability_vs_apm"&gt;Observability vs. APM&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You may wonder if observability is just a new and flashier name for application performance monitoring. In fact, observability has a number of advantages for the end user over traditional APM:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Vastly reduced vendor lock-in&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open specification wire protocols&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open source client components&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Standardized architecture patterns&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Increasing quantity and quality of open source backend components&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In the next two years, you should expect to see a further strengthening of key open source observability projects, as well as market consolidation onto a few segment leaders.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_java_observability"&gt;Java observability&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Java is hugely important to Red Hat&amp;#8217;s users, and there are particular challenges to the adoption of observability in Java stacks. Fundamentally, Java technology was designed for a world where JVMs ran on bare metal in data centers, so the applications don&amp;#8217;t easily map to tools designed for containerization.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The world is changing, however. Cloud-native deployments&amp;#8212;&amp;#8203;especially containers&amp;#8212;&amp;#8203;are here and being adopted quickly, albeit at varying rates across different parts of the industry.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The traditional Java application lifecycle consists of a number of phases: bootstrap, intense class loading, warmup (with JIT compilation), and finally a long-lived steady state (lasting for days or weeks) with relatively little class loading or JIT. This model is challenged by cloud deployments, where containers might live for much shorter time periods and cluster sizes might be dynamically readjusted.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As it moves to containers, Java has to ensure that it remains competitive along several key axes, including footprint, density, and startup time. Fortunately, ongoing research and development within OpenJDK is trying to make sure that the platform continues to optimize for these characteristics&amp;#8212;&amp;#8203;and Red Hat is a key contributor to this work.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you&amp;#8217;re a Java developer looking to adapt to this new world, the first thing to do is prepare your application or company plan for observability. OpenTelemetry is likely to be the library of choice for many developers for both tracing and metrics. Existing libraries, especially &lt;a href="https://micrometer.io/"&gt;Micrometer&lt;/a&gt;, are also likely to have a prominent place in the landscape. In fact, interoperability with existing components within the Java ecosystem is a key goal for OpenTelemetry.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_status_and_roadmap"&gt;Status and roadmap&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;OpenTelemetry has several subprojects that are at different levels of maturity.&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The Distributed Tracing specification is at v1.0 and is being widely deployed into production systems. It replaces OpenTracing completely, and the OpenTracing project has officially been &lt;a href="https://medium.com/opentracing/opentracing-has-been-archived-fb2848cfef67"&gt;archived&lt;/a&gt;. The Jaeger project, one of the most popular distributing tracing backends, has also discontinued its client libraries and will default to OpenTelemetry protocols going forward.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The OpenTelemetry Metrics project is not quite as advanced, but it is approaching v1.0 and General Availability (GA). At time of writing, the protocol is at the Stable stage and the API is at Feature Freeze. It is anticipated that the project might reach v1.0/GA during April 2022.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Finally, the Logging specification is still in Draft stage and is not expected to reach v1.0 until late 2022. There is still a certain acknowledged amount of work to do on the spec, and participation is actively being sought by the working groups.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Overall, OpenTelemetry as a whole will be considered to be v1.0/GA when the Metrics standard reaches v1.0 alongside Tracing.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The major takeaways are that observability is reaching more and more developers and is noticeably gathering steam. Some analysts even anticipate that OpenTelemetry formats will become the largest single contributor to observability traffic as early as 2023.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_red_hat_and_the_opentelemetry_collector"&gt;Red Hat and the OpenTelemetry Collector&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As the industry starts to embrace OpenTelemetry, it&amp;#8217;s important to help users decide what to do with all the telemetry data that is being generated. One key piece is the &lt;a href="https://opentelemetry.io/docs/collector/"&gt;OpenTelemetry Collector&lt;/a&gt;. This component runs as a network service that can receive, proxy, and transform data. It enables users to keep data and process it internally, or forward it to a third party.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Collector helps solve one of the major hurdles to adoption faced by a new standard such as OpenTelemetry: Dealing with legacy applications and with infrastructure that already exists. The Collector can understand and speak to many different legacy protocols and payloads, and can translate them into OpenTelemetry-supported protocols. In turn, these open protocols can be consumed by a vast number of vendors who embrace the specification. This unification of older protocols is a major shift in the observability space, and is going to offer a level of flexibility that we haven&amp;#8217;t seen before.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Red Hat is deeply committed to the OpenTelemetry community and has released the OpenTelemetry Collector as a component of the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; Container Platform, branded as &lt;a href="https://catalog.redhat.com/software/operators/detail/5ec54a5c78e79e6a879fa271"&gt;OpenShift distributed tracing data collection&lt;/a&gt;. This helps our users take advantage of all the capabilities the OpenTelemetry Collector has to offer in order to provide a better, more open approach to observability.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The core architectural capabilities of the Collector can be summarized as follows:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Usability:&lt;/strong&gt; A reasonable default configuration that works out of the box&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Performant under varying loads and configurations&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Observability:&lt;/strong&gt; A good example of an observable service&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensibility:&lt;/strong&gt; Customizable without touching the core code&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unification:&lt;/strong&gt; A single codebase that supports traces, metrics, and logs&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability is an emerging set of ideas and DevOps practices that help handle the complexity of modern architectures and applications, rather than any specific set of products.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability absorbs and extends classic monitoring systems, and helps teams identify the root cause of issues. More broadly, it allows stakeholders to answer questions about their application and business, including forecasting and predictions about what could go wrong.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A diverse collection of tools and technologies are in use, which leads to a large matrix of possible deployments. This has architectural consequences, so teams need to understand how to set up their observability systems in a way that works for them.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;One key technology is OpenTelemetry. It is rapidly gaining popularity, but is still maturing, and the open source groups and standards need more participation, especially by end users.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The post &lt;a href="/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help" title="Observability in 2022: Why it matters and how OpenTelemetry can help"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Ben Evans</dc:creator><dc:date>2022-04-12T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.8.0.Final released - New REST layer by default, GraalVM 22.0 and much more!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-8-0-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-8-0-final-released/</id><updated>2022-04-12T00:00:00Z</updated><published>2022-04-12T00:00:00Z</published><summary type="html">2.8.0.Final comes with a lot of refinements and new features: Move Assertj outside of our BOM New REST layer by default GraalVM 22.0 Support for OIDC Proof Of Key for Code Exchange (PKCE) QuarkusTransaction API Elasticsearch Dev Services And much more! Migration Guide To migrate from 2.7, please refer to...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-04-12T00:00:00Z</dc:date></entry><entry><title type="html">Rise of J2Cl: Java web development after GWT￼</title><link rel="alternate" href="https://blog.kie.org/2022/04/rise-of-j2cl-java-web-development-after-gwt.html" /><author><name>Dmitrii Tikhomirov</name></author><id>https://blog.kie.org/2022/04/rise-of-j2cl-java-web-development-after-gwt.html</id><updated>2022-04-11T18:08:55Z</updated><content type="html">It looks like 15 years of GWT are coming to the end, and besides that web development has dramatically changed since 2006. There is now no chaos of conflicting browser implementations that require to run multiple permutations each. At the same time modern web development frameworks are far from ideal. For instance, a very strong advantage of GWT was the ecosystem around Maven – the stability and usability of this solution was incredible, especially when big teams worked on large projects. Google, the main developer of GWT, left the project and started J2CL, the successor of GWT, which takes the very best practices to a new level. Their documentation calls it out as being used in many high performance projects such as Gmail, Inbox, Docs, Slides, and Calendar. Initially J2CL was developed to be used in the Bazel environment. After several years of hard work the community, led by Colin Alworth, released the first public J2CL version for Maven – j2cl-maven-plugin. So let’s take a look at what it is and how it works. J2CL AND CLOSURE COMPILER J2CL is responsible for only one task – to transpile a set of Java classes into a set of JavaScript files. Google’s Closure Compiler is responsible for merging this set of javascripts into one executable JS script, its optimization and minification. Closure Compiler is extremely efficient in minification and optimization of the JavaScript, it simply has no competitors. GENERATING OUR FIRST J2CL PROJECT Let’s start from a simple one module application. Luckily for us, we can generate it from a pre-build archetype. Download the archetype if you don’t have it:  mvn org.apache.maven.plugins:maven-dependency-plugin:get \ -DrepoUrl=https://repo.vertispan.com/j2cl/ \ -Dartifact=com.vertispan.j2cl.archetypes:j2cl-archetype-simple:0.19 Now we can generate a simple application: mvn archetype:generate -DarchetypeGroupId=com.vertispan.j2cl.archetypes \ -DarchetypeArtifactId=j2cl-archetype-simple \ -DarchetypeVersion=0.19 Let’s take a look at the result: ├── pom.xml └── src ├── main │ ├── java │ │ └── org │ │ └── treblereel │ │ └── j2cl │ │ ├── App.java │ │ └── App.native.js │ └── webapp │ ├── WEB-INF │ │ └── web.xml │ ├── css │ │ └── simpleapp.css │ └── index.html └── test └── java └── org └── treblereel └── j2cl └── AppTest.java – App.java is a starting point of our application and there’s is one point I have to highlight below. – App.native.js used to specify how to start our application to the Closure Compiler, because it knows nothing about it. Usage of native.js is a very large topic and a separate article can be written about it. – AppTest.java is just a J2CL-compatible unit test that runs in HtmlUnit, it’s also possible to use ChromeDriver to run it in a real browser but it takes longer. – pom.xml – here the only interesting part for us is the j2cl-maven-plugin section. For now it contains only the &lt;compilationLevel&gt; declaration used to set which level of optimization we are going to use during the compilation of the project. ADVANCED is the most efficient one, so Closure Compiler does aggressive renaming, dead code removal, global inlining and so on. But in some cases Closure Compiler needs our help and care – we have to declare which properties or methods should not be removed or renamed. BUNDLE is less strict and better suitable for development because each compilation round takes less time compared to ADVANCED.  RUNNING AND BUILDING OUR J2CL APPLICATION j2cl-maven-plugin allows us to run our application in the development mode with build-in hot code reload and source map debug. To start devmode, run the following command in the terminal: &gt; mvn j2cl:watch When the application started, run the following command in the second terminal: &gt; mvn jetty:run There is no need to run ‘mvn clean’ each time because J2CL will recompile everything from scratch, and we can reuse the results from the previous run. Moreover, there is an option to use global cache between several projects to reduce compilation time. To build ‘.war’ we should run ‘mvn package’, there is nothing new here, everything is pretty familiar to GWT developers. OK, WHAT IS NEW COMPARED TO GWT * GWT modules are gone, yes, no more modules. So J2CL will try to compile direct and transitive dependencies from pom.xml, that is why we should set ‘provided scope’ to annotation processors and shade them. * GWT.create gone as well * GWT generators are gone, now we should use APT-based generators. * What about GWT components and widgets we used before ? Most of them have been ported to J2CL. * Does @GwtIncompatible work? Yes, it is still here. AND WHAT IS THE VALUE OF IT FOR US? Right now we are focused on migration of our existing projects from gwt2 to j2cl. There are many libraries that have been migrated to j2cl and there are many libraries that support gwt2 and j2cl. I would like to highlight elemental2-* wrappers, DominoKit, Nalu, and many others. Gwt2 has been ported as gwtproject – set of migrated modules. Errai framework – the core component of our applications has been re-implemented as Crysknife project. In the upcoming posts i am going to address several topics: * Existing libraries and frameworks that are J2CL-compatible * Defines – how to propagate variables to J2CL * How we can improve generated code with native.js * Externs – why do we need them and how to write our own externs * Semi-reflection  * Interoperability with TS * And maybe many more The post appeared first on .</content><dc:creator>Dmitrii Tikhomirov</dc:creator></entry><entry><title>Introduction to the Node.js reference architecture, Part 8: TypeScript</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript" /><author><name>Dominic Harries</name></author><id>77b82ab1-239d-4d61-9ee6-f3ccffd31756</id><updated>2022-04-11T07:00:00Z</updated><published>2022-04-11T07:00:00Z</published><summary type="html">&lt;p&gt;One of the key choices you make when building an enterprise &lt;a href="/topics/nodejs"&gt;Node.js&lt;/a&gt; application is whether to use plain &lt;a href="/topics/javascript"&gt;JavaScript&lt;/a&gt; or a dialect that supports type validation. While participating in the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/"&gt;Node.js reference architecture effort&lt;/a&gt;, we've pulled together many internal Red Hat and IBM teams to discuss our experience with using both plain JavaScript and &lt;a href="https://www.typescriptlang.org"&gt;TypeScript&lt;/a&gt;. Our projects seem to be split between the two, and it's often "love it or hate it" when using types with JavaScript.&lt;/p&gt; &lt;p&gt;TypeScript is in widespread use, particularly among enterprise developers coming from other languages such as &lt;a href="/topics/enterprise-java"&gt;Java&lt;/a&gt;. TypeScript was recently voted the third most loved programming language in StackOverflow's &lt;a href="https://insights.stackoverflow.com/survey/2021#most-loved-dreaded-and-wanted-language-love-dread"&gt;annual developer survey&lt;/a&gt;—far ahead of JavaScript itself.&lt;/p&gt; &lt;p&gt;This article covers why you might want to use TypeScript and how to get started, along with an introduction to the recommendations in the Node.js reference architecture. As with all our Node.js reference architecture recommendations, we focus on defining a set of good and reliable default choices. Some teams will deviate from the recommendations based on their assessment of what best fits their use case.&lt;/p&gt; &lt;p&gt;Read the series so far:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;Choosing web frameworks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;Code Coverage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Part 8: TypeScript&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why use TypeScript?&lt;/h2&gt; &lt;p&gt;JavaScript has come a long way from its humble beginnings as a lightweight scripting language inside the browser. Technologies such as Node.js have propelled it to become one of the leading languages for back-end development.&lt;/p&gt; &lt;p&gt;But as codebases grow in size, it can be increasingly difficult to track down errors and keep track of the data flowing through an application. That's true in any language, but it's a particular problem in weakly typed languages like JavaScript.&lt;/p&gt; &lt;p&gt;TypeScript is designed to address this problem. By adding type annotations to variables, TypeScript can help to document the data a program uses, catch errors, and give developers confidence that they can change code in one place without breaking other parts of their codebase.&lt;/p&gt; &lt;p&gt;Many code editors now have excellent TypeScript support. This support enables code completion, immediate feedback on type errors, powerful automatic refactoring, and other useful features. For instance, &lt;a href="https://code.visualstudio.com"&gt;Visual Studio Code&lt;/a&gt; is a widely used editor that comes with extensive &lt;a href="https://code.visualstudio.com/docs/languages/typescript"&gt;support for TypeScript&lt;/a&gt;. The TypeScript wiki contains a &lt;a href="https://github.com/Microsoft/TypeScript/wiki/TypeScript-Editor-Support"&gt;list of other editors&lt;/a&gt; with TypeScript support.&lt;/p&gt; &lt;p&gt;Most popular third-party JavaScript libraries now ship with TypeScript type definitions or make them available via the &lt;a href="https://github.com/DefinitelyTyped/DefinitelyTyped"&gt;Definitely Typed&lt;/a&gt; repository.&lt;/p&gt; &lt;p&gt;These capabilities have caused TypeScript to explode in popularity.&lt;/p&gt; &lt;h2&gt;Get started with TypeScript&lt;/h2&gt; &lt;p&gt;TypeScript has been designed to be easy to adopt, even for existing JavaScript projects. You can incrementally enable TypeScript a single file at a time while leaving the rest of your project in JavaScript.&lt;/p&gt; &lt;p&gt;To demonstrate this flexibility, we'll port a very simple Node.js application to TypeScript. The application consists of a single JavaScript file named &lt;code&gt;fill.js&lt;/code&gt; in the project's &lt;code&gt;src&lt;/code&gt; directory. The code fills an array with a value:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;function fillArray(len, val) { const arr = []; for (let i = 0; i &amp;lt; len; i++) { arr.push(val); } return arr; } module.exports = { fillArray };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step one is to install a TypeScript compiler. Because Node.js does not natively understand TypeScript files, they must be compiled to JavaScript before they can be executed. The compilation from TypeScript to JavaScript is called &lt;em&gt;transpiling&lt;/em&gt;. There are multiple transpilers available (see the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/development/typescript"&gt;reference architecture&lt;/a&gt; for details), but we'll use the standard TypeScript compiler &lt;code&gt;tsc&lt;/code&gt;. Install it as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npm install --save-dev typescript&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're using any built-in Node.js modules, you also need the types for these:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npm install --save-dev @types/node&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The compilation process is configured using a &lt;code&gt;tsconfig.json&lt;/code&gt; file. This configuration controls all the parameters for TypeScript compilation. The Node.js community maintains a recommended configuration that you can install as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npm install --save-dev @tsconfig/node16&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are using a Node.js version older than 16, you can check &lt;a href="https://github.com/tsconfig/bases"&gt;the list of bases&lt;/a&gt; for recommended configurations compatible with older versions.&lt;/p&gt; &lt;p&gt;Add Node.js options to your &lt;code&gt;tsconfig.json&lt;/code&gt; file as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "extends": "@tsconfig/node16/tsconfig.json", "compilerOptions": { "allowJs": true, "strict": false, "outDir": "./build" }, "include": ["./src/**/*"] }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This configuration specifies that all files under the &lt;code&gt;src&lt;/code&gt; directory should be compiled and put in the build directory. It also allows your source files to stay written in JavaScript (these will be copied across to the build directory without modification) and disables strict mode (further details later on strict mode). There are many further options that you can set—please see our recommendations in the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/development/typescript"&gt;reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To run the compile, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npx tsc&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this simple example, because we haven't defined any data types, the compiler created an identical &lt;code&gt;fill.js&lt;/code&gt; file in the build directory.&lt;/p&gt; &lt;h3&gt;Adding some TypeScript&lt;/h3&gt; &lt;p&gt;Node.js supports two module systems:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;CommonJS: The traditional format, which uses the &lt;code&gt;require&lt;/code&gt; keyword to import code and &lt;code&gt;module.exports&lt;/code&gt; to export it.&lt;/li&gt; &lt;li&gt;ES modules: A newer format using the &lt;code&gt;import&lt;/code&gt; keyword to import code and the &lt;code&gt;export&lt;/code&gt; keyword to export it. This format is supported by both Node.js and web browsers.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;TypeScript supports only the ES module format, so in addition to renaming your example file to &lt;code&gt;src/fill.ts&lt;/code&gt;, you need to update its export:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;export function fillArray(len, val) { const arr = []; for (let i = 0; i &amp;lt; len; i++) { arr.push(val); } return arr; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This code now compiles successfully, even though you haven't added any types. This is because strict mode is set to false in the &lt;code&gt;tsconfig.json&lt;/code&gt; file. If you set the mode to &lt;code&gt;true&lt;/code&gt;, you will see an error like the following when you compile:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; src/fill.ts:1:27 - error TS7006: Parameter 'len' implicitly has an 'any' type. src/fill.ts:1:32 - error TS7006: Parameter 'val' implicitly has an 'any' type. &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You can add some annotations to the argument list in the first line to fix these errors:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;export function fillArray(len: number, val: any) { const arr = []; for (let i = 0; i &amp;lt; len; i++) { arr.push(val); } return arr; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The changes make the compilation succeed. Even better, if you accidentally forget which way round the parameters go and call the method like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;console.log(fillArray("-", 5));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;TypeScript gives another helpful error:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;error TS2345: Argument of type 'string' is not assignable to parameter of type 'number'.&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;We recommend enabling strict mode for new projects, but when migrating existing projects it may be easier to leave the mode disabled.&lt;/p&gt; &lt;p&gt;Many editors can be configured to immediately show TypeScript errors rather than waiting until you run the compiler. Editors may also offer other advanced features such as code completion and automatic refactoring.&lt;/p&gt; &lt;h2&gt;Node.js reference architecture recommendations&lt;/h2&gt; &lt;p&gt;Teams need to make a number of key choices when using TypeScript. These include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Should transpilers be used? If so, which ones?&lt;/li&gt; &lt;li&gt;What should be shipped: the original files, or the transpiled versions?&lt;/li&gt; &lt;li&gt;What TypeScript options and configuration should be used?&lt;/li&gt; &lt;li&gt;How should types for npm packages be published?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The Node.js reference architecture contains further recommendations, including how to use TypeScript with tools such as &lt;code&gt;nodemon&lt;/code&gt; and best practices for deployment based on the experience our team has gained through deployments within Red Hat, IBM, and our customers.&lt;/p&gt; &lt;p&gt;Those recommendations are well defined in the Node.js reference architecture, so instead of repeating them here, we encourage you to head over to the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/development/typescript"&gt;TypeScript section of the reference architecture itself&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What's next?&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Node.js reference architecture series&lt;/a&gt;. While you wait for the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you'll see the work we've already done and the kinds of topics you can look forward to in the future.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="topics/nodejs"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript" title="Introduction to the Node.js reference architecture, Part 8: TypeScript"&gt;Introduction to the Node.js reference architecture, Part 8: TypeScript&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Dominic Harries</dc:creator><dc:date>2022-04-11T07:00:00Z</dc:date></entry><entry><title type="html">What’s new in Jakarta EE 10</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/whats-new-in-jakarta-ee-10/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/whats-new-in-jakarta-ee-10/</id><updated>2022-04-08T15:56:30Z</updated><content type="html">Jakarta EE 10 is the first major release of Jakarta EE since the “jakarta” namespace update. Many of the component specifications are introducing Major or Minor version updates that are going to reflect in the implementation APIs. Let’s learn in this article what we can expect from the upcoming new major release. Project status Firstly, ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>3 ways to install a database with Helm charts</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/07/3-ways-install-database-helm-charts" /><author><name>Wanja Pernath</name></author><id>f6ca6909-f9d0-4fc8-873e-9f17b651b575</id><updated>2022-04-07T07:00:00Z</updated><published>2022-04-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="/topics/helm/all"&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Helm uses a packaging format called &lt;em&gt;charts,&lt;/em&gt; which include all of the Kubernetes resources that are required to deploy an application, such as deployments, services, ingress, and so on. Helm charts are very useful for installing applications and performing upgrades on a Kubernetes cluster.&lt;/p&gt; &lt;p&gt;In chapter 3 of my e-book &lt;a href="/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;&lt;em&gt;Getting GitOps: A Practical Platform with OpenShift, Argo CD and Tekton&lt;/em&gt;&lt;/a&gt;, I discuss the basics of creating and using Helm charts. I also dig into the the use case of creating a post-install and post-upgrade job.&lt;/p&gt; &lt;p&gt;However, that chapter provided a very basic example that focused only on what's necessary to create and deploy a Helm chart. This article will demonstrate some more advanced techniques to create a chart that could be installed more than once in the same namespace. It also shows how you could easily install a dependent database with your chart.&lt;/p&gt; &lt;p&gt;The source code for this example can be found in the &lt;a href="https://github.com/wpernath/book-example/tree/main/better-helm"&gt;GitHub repository that accompanies my book&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The use case: How to install a dependent database with a Helm chart&lt;/h2&gt; &lt;p&gt;Chapter 1 of my book outlines the &lt;a href="/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;-based &lt;code&gt;person-service&lt;/code&gt;, a simple REST API service that reads and writes personal data from and into a PostgreSQL database. A Helm chart packages this service, and needs to provide all the dependencies necessary to successfully install it. As discussed in that chapter, you have three options to achieve that goal:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use the corresponding OpenShift template to install the necessary PostgreSQL database&lt;/li&gt; &lt;li&gt;Use the &lt;a href="https://github.com/CrunchyData/postgres-operator/"&gt;CrunchyData Postgres Operator&lt;/a&gt; (or any other Operator-defined PostgreSQL database extension) for the database&lt;/li&gt; &lt;li&gt;Install a dependent Helm chart, such as the &lt;a href="https://artifacthub.io/packages/helm/bitnami/postgresql"&gt;PostgreSQL chart by Bitnami&lt;/a&gt;, with your chart&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No matter which route you take, however, you also need to ensure that your chart can be installed multiple times on each namespace. So let's tackle that task first.&lt;/p&gt; &lt;h2&gt;Make the chart installable multiple times in the same namespace&lt;/h2&gt; &lt;p&gt;The most crucial step for making your chart installable multiple times in the same namespace is to use generated names for all the manifest files. Therefore, you need an object called &lt;code&gt;Release&lt;/code&gt; with the following properties:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Name&lt;/code&gt;: The name of the release&lt;/li&gt; &lt;li&gt;&lt;code&gt;Namespace&lt;/code&gt;: Where you are going to install the chart&lt;/li&gt; &lt;li&gt;&lt;code&gt;Revision&lt;/code&gt;: The revision number of this release (starts at 1 on install, and each update increments it by one)&lt;/li&gt; &lt;li&gt;&lt;code&gt;IsInstall&lt;/code&gt;: &lt;code&gt;true&lt;/code&gt; if it's an installation process&lt;/li&gt; &lt;li&gt;&lt;code&gt;IsUpgrade&lt;/code&gt;: &lt;code&gt;true&lt;/code&gt; if it's an upgrade process&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to make sure that your chart installation won't conflict with any other installations in the same namespace, do the following:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-config labels: app.kubernetes.io/part-of: {{ .Release.Name }}-chart data: APP_GREETING: |- {{ .Values.config.greeting | default "Yeah, it's openshift time" }} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This creates a &lt;code&gt;ConfigMap&lt;/code&gt; with the name of the release, followed by a dash, followed by &lt;code&gt;config&lt;/code&gt;. Of course, you now need to make sure that the &lt;code&gt;ConfigMap&lt;/code&gt; is being read by the deployment accordingly:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;- image: "{{ .Values.deployment.image }}:{{ .Values.deployment.version }}" envFrom: - configMapRef: name: {{ .Release.Name }}-config [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are updating all the other manifest files in your Helm's &lt;code&gt;templates&lt;/code&gt; folder, you can install your chart multiple times.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm install person-service1 &amp;lt;path to chart&amp;gt; $ helm install person-service2 &amp;lt;path to chart&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With that task out of the way, we can now consider each of the three potential approaches outlined above in turn.&lt;/p&gt; &lt;h2&gt;Install the database via an existing OpenShift template&lt;/h2&gt; &lt;p&gt;By far easiest way to install a PostgreSQL database in an OpenShift namespace is by using an OpenShift template. We did it several times in my book. The call is simple:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc new-app postgresql-persistent \ -p POSTGRESQL_USER=wanja \ -p POSTGRESQL_PASSWORD=wanja \ -p POSTGRESQL_DATABASE=wanjadb \ -p DATABASE_SERVICE_NAME=wanjaserver &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But how could you automate this process? There is no way to execute this call from within a Helm chart installation. (Well, you could do it by using a pre-install hook, but that would be quite ugly.)&lt;/p&gt; &lt;p&gt;Fortunately, the OpenShift client has a function called &lt;code&gt;process&lt;/code&gt; that processes a template. The result of this call is a list of YAML objects that can then be installed into OpenShift.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc process postgresql-persistent -n openshift -o yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're piping the result into a new file, you would get something like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: v1 kind: List items: - apiVersion: v1 kind: Secret metadata: labels: template: postgresql-persistent-template name: postgresql stringData: database-name: sampledb database-password: KSurRUMyFI2fiVpx database-user: user0U4 - apiVersion: v1 kind: Service [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're not happy with the default parameters for username, password, and database name, call the process function with the &lt;code&gt;-p PARAM=VALUE&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc process postgresql-persistent -n openshift -o yaml \ -p POSTGRESQL_USER=wanja \ -p POSTGRESQL_PASSWORD=wanja \ -p POSTGRESQL_DATABASE=wanjadb \ -p DATABASE_SERVICE_NAME=wanjaserver &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Place the resulting file into your chart's &lt;code&gt;templates&lt;/code&gt; folder, and it will be used to install the database. If you have a closer look at the file, you can see that it's using &lt;code&gt;DATABASE_SERVICE_NAME&lt;/code&gt; as manifest names for its &lt;code&gt;Service&lt;/code&gt;, &lt;code&gt;Secret&lt;/code&gt;, and &lt;code&gt;DeploymentConfig&lt;/code&gt; objects, which would make it impossible to install your resulting chart more than once into any namespace.&lt;/p&gt; &lt;p&gt;If you're providing the string &lt;code&gt;-p DATABASE_SERVICE_NAME='pg-{{ .Release.Name }}'&lt;/code&gt; instead of the fixed string &lt;code&gt;wanjaserver&lt;/code&gt;, then this will be used as the object name for these manifest files. However, if you try to install your Helm chart now, you'll get some verification error messages. This is because &lt;code&gt;oc process&lt;/code&gt; generates some top-level status fields that the Helm parser does not understand, so you need to remove them.&lt;/p&gt; &lt;p&gt;The only thing you now need to do is to connect your &lt;code&gt;person-service&lt;/code&gt; deployment with the corresponding database instance. Simply add the following entries to the &lt;code&gt;env&lt;/code&gt; section of your &lt;code&gt;Deployment.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] env: - name: DB_host value: pg-{{ .Release.Name }}.{{ .Release.Namespace }}.svc - name: DB_dbname valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-name - name: DB_user valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-user - name: DB_password valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-password [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your Helm chart is now ready to be packaged and installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package better-helm/with-templ $ helm upgrade --install ps1 person-service-templ-0.0.10.tgz &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately, one of the resulting manifest files is a &lt;code&gt;DeploymentConfig&lt;/code&gt;, which would only work on &lt;a href="/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. As a result, this chart can't be installed on any other Kubernetes distribution. So let's discuss other options.&lt;/p&gt; &lt;h2&gt;Install a Kubernetes Operator with your chart&lt;/h2&gt; &lt;p&gt;Another way to install a dependent database with your Helm chart is to look for a Kubernetes Operator on &lt;a href="https://operatorhub.io/"&gt;OperatorHub&lt;/a&gt;. If your cluster already has an Operator Lifecycle Manager (OLM) installed (as all OpenShift clusters do), then the only thing you need to do is create a &lt;code&gt;Subscription&lt;/code&gt; that describes your desire to install an Operator.&lt;/p&gt; &lt;p&gt;For example, to install the community operator by CrunchyData into OpenShift, you would need to create the following file:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: postgresql-operator namespace: openshift-operators spec: channel: v5 name: postgresql source: community-operators sourceNamespace: openshift-marketplace &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you put this file into the &lt;code&gt;crds&lt;/code&gt; folder of your Helm chart, Helm takes care of installing the Operator before it processes the template files of the chart. Please note, however, that Helm will &lt;em&gt;never&lt;/em&gt; uninstall the custom resource definitions, so the Operator will stay on the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;If you place the following file into the &lt;code&gt;templates&lt;/code&gt; folder of the chart, your PostgreSQL database instance will be ready to use:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: postgres-operator.crunchydata.com/v1beta1 kind: PostgresCluster metadata: name: {{ .Release.Name }}-db labels: app.kubernetes.io/part-of: {{ .Release.Name }}-chart spec: image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres:centos8-13.5-0 postgresVersion: 13 instances: - name: instance1 dataVolumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi backups: pgbackrest: image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.36-0 repos: - name: repo1 volume: volumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Of course, you now need to make sure that your &lt;code&gt;person-service&lt;/code&gt; is able to connect to this PostgreSQL instance. Simply add a &lt;code&gt;secretRef&lt;/code&gt; to the deployment file with the following content:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] envFrom: - secretRef: name: {{ .Release.Name }}-db-pguser-{{ .Release.Name }}-db prefix: DB_ [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will map all values of the &lt;code&gt;PostgresCluster&lt;/code&gt; secret to your deployment with a prefix of &lt;code&gt;DB_&lt;/code&gt;, which is exactly what you need.&lt;/p&gt; &lt;p&gt;Now your chart is ready to be packaged and can be installed in any OpenShift namespace:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package with-crds $ helm install ps1 person-service-crd-0.0.10.tgz $ helm uninstall ps1 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install the database by adding a subchart dependency&lt;/h2&gt; &lt;p&gt;The last option is to use a subchart within your chart. For this scenario, Helm has a &lt;a href="https://helm.sh/docs/topics/charts/#chart-dependencies"&gt;dependency management system&lt;/a&gt; that makes it easier for you as a chart developer to use third-party charts. The example that follows makes use of the &lt;a href="https://artifacthub.io/packages/helm/bitnami/postgresql"&gt;Bitnami PostgreSQL chart&lt;/a&gt;, which you can find on &lt;a href="https://artifacthub.io"&gt;ArtifactHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To start, you have to change the &lt;code&gt;Chart.yaml&lt;/code&gt; file to add the external dependency. With the following lines, you can add the dependency to the Bitnami PostgreSQL database with the version &lt;code&gt;11.1.3&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;dependencies: - name: postgresql repository: https://charts.bitnami.com/bitnami version: 11.1.3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want to define properties from within your &lt;code&gt;values.yaml&lt;/code&gt; file, you simply need to use the name of the chart as the first parameter in the tree; in this case, it is &lt;code&gt;postgresql&lt;/code&gt;. You can then add all necessary parameters below that key:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;postgresql: auth: username: wanja [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, you need to have a look at the documentation of the Bitnami chart to understand how to use it in your target environment (OpenShift, in this case). Unfortunately, as of the time of this writing, the current documentation is a bit outdated, so you would not be able to install your chart without digging further into the &lt;code&gt;values.yaml&lt;/code&gt; file Bitnami supplies to see which security settings you have to set in order to use it with OpenShift's strong enterprise security.&lt;/p&gt; &lt;p&gt;To save you the trouble, I've put together this minimum list of settings you would need to use:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;postgresql: auth: username: wanja password: wanja database: wanjadb primary: podSecurityContext: enabled: false fsGroup: "" containerSecurityContext: enabled: false runAsUser: "auto" readReplicas: podSecurityContext: enabled: false fsGroup: "" containerSecurityContext: enabled: false runAsUser: "auto" volumePermissions: enabled: false securityContext: runAsUser: "auto" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The final step is to make sure that your deployment is able to connect to this database.&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] env: - name: DB_user value: wanja - name: DB_password valueFrom: secretKeyRef: name: {{ .Release.Name }}-postgresql key: password - name: DB_dbname value: wanjadb - name: DB_host value: {{ .Release.Name }}-postgresql.{{ .Release.Namespace }}.svc [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to package your chart. Because you're depending on a third-party chart, you need to use the &lt;code&gt;-u&lt;/code&gt; option, which downloads the dependencies into the &lt;code&gt;charts&lt;/code&gt; folder of your Helm chart.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package -u better-helm/with-subchart $ helm install ps1 person-service-sub.0.0.11.tgz &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Using Helm charts for your own projects is quite easy, even if you need to make sure certain dependencies are being installed as well. Thanks to Helm's dependency management, you can easily use subcharts with your charts. And thanks to the flexibility of Helm, you can also either use a (processed) template or quickly install a Kubernetes Operator before proceeding.&lt;/p&gt; &lt;p&gt;Check out these articles to learn more about Helm:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/articles/2021/05/24/deploy-helm-charts-jenkins-cicd-red-hat-openshift-4"&gt;Deploy Helm charts with Jenkins CI/CD in Red Hat OpenShift 4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/articles/2021/10/19/deploy-java-application-using-helm-part-1"&gt;Deploy a Java application using Helm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And for a more in-depth look at the example explored here, check out my e-book, &lt;a href="/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;&lt;em&gt;Getting GitOps: A Practical Platform with OpenShift, Argo CD and Tekton&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="/articles/2022/04/07/3-ways-install-database-helm-charts" title="3 ways to install a database with Helm charts"&gt;3 ways to install a database with Helm charts&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Wanja Pernath</dc:creator><dc:date>2022-04-07T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 07 April 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-04-07.html" /><category term="quarkus" /><category term="kubernetes" /><category term="java" /><category term="infinispan" /><category term="wildfly" /><category term=".net core" /><category term="hot rod" /><category term="cloud-native" /><category term="openshift" /><category term="quarkus camel" /><category term="apache camel" /><category term="atlasmap" /><category term="microservice" /><category term="events" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-04-07.html</id><updated>2022-04-07T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kubernetes, java, infinispan, wildfly, .net core, hot rod, cloud-native, openshift, quarkus camel, apache camel, atlasmap, microservice, events"&gt; &lt;h1&gt;This Week in JBoss - 07 April 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hi everyone! It’s great to be back and bringing you another edition of the JBoss Editorial. As always there’s a lot of exciting news and updates from JBoss communities so let’s dive in.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/vscode-quarkus-1.10.0/"&gt;Quarkus Tools for Visual Studio Code 1.10.0&lt;/a&gt; - Quarkus Tools for Visual Studio Code 1.10.0 is available on the &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-quarkus"&gt;VS Code Marketplace&lt;/a&gt; and &lt;a href="https://open-vsx.org/extension/redhat/vscode-quarkus"&gt;Open VSX&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/03/24/WildFly26-1-Beta-Released/"&gt;WildFly 26.1 Beta1&lt;/a&gt; - WildFly and WildFly Preview 26.1.0.Beta1 releases are available for download.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/03/29/WildFly-s2i-26-1-Beta1-Released/"&gt;WildFly S2I builder and runtime images&lt;/a&gt; - S2I images for WildFly 26.1 Beta are on &lt;a href="quay.io/wildfly"&gt;quay.io/wildfly&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/04/kogito-1-19-0-released.html"&gt;Kogito 1.19.0&lt;/a&gt; - Kogito 1.19.0 is now available.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_net_core_client_for_infinispan"&gt;.Net core client for Infinispan&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2022/03/22/dotnetcore-0-0-3-beta"&gt;Infinispan .Net Core Client&lt;/a&gt;, by Vittorio Rigamonti&lt;/p&gt; &lt;p&gt;Vittorio Rigamonti has made a beta release of his .Net core client that allows access to remote caches on Infinispan clusters. The client provides .Net core applications with the performance benefits of in-memory data storage. Combined with the Protobuf data encoding Vittorio’s client also gives .Net core applications data interoperability with Java applications that can access the same Infinispan caches. Using Protobuf with the .Net core client lets you perform remote queries as an added bonus!&lt;/p&gt; &lt;p&gt;Go check out Vittorio’s post and use his &lt;a href="https://github.com/infinispan/Infinispan.Hotrod.Core/tree/main/Infinispan.Hotrod.Application"&gt;sample application&lt;/a&gt; to get started coding. While you’re at it, you might find Vittorio’s other blog post on &lt;a href="https://infinispan.org/blog/2022/01/21/dotnet-core-query"&gt;performing remote queries&lt;/a&gt; useful.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_inside_out_event_streaming_with_microservice_architectures"&gt;Inside out event streaming with microservice architectures&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.ofbizian.com/2022/04/turning-microservices-inside-out.html"&gt;Turning Microservices Inside-Out&lt;/a&gt;, by Bilgin Ibryam&lt;/p&gt; &lt;p&gt;This excellent read from Bilgin starts with recent ideas about the design of relational databases and their place in the event-driven world of microservice architectures. Bilgin makes his point that, rather than replacing relational databases with event streams, the inside out design principle is better applied to the service level. Using traditional databases with an event-driven approach allows you to combine the best of both technologies but requires a deliberate focus on APIs. From there Bilgin explains the flow of events through outbound and inbound APIs and how a "connecting tissue" such as Debezium can help you effectively bring relational databases into event-driven microservices.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_camel_quarkus_the_swiss_knife_of_integration"&gt;Camel Quarkus: the Swiss knife of integration&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/camel-quarkus-effortless-apis/"&gt;Riding Camel Quarkus: effortless APIs&lt;/a&gt;, by Bruno Meseguer&lt;/p&gt; &lt;p&gt;Bruno showcases Camel Quarkus in an extensive post that will leave anyone working on service integration how they could live without it.&lt;/p&gt; &lt;p&gt;Using the OpenAPI specification as an example Bruno explains how to use Camel Quarkus to connect to an HTTP service and transform data in a few effortless steps. There’s a lot to discover in his post and Bruno even walks you through a second iteration of his application in which he reduces the path to integration even further.&lt;/p&gt; &lt;p&gt;I think by the end Bruno proves his point, that Camel Quarkus really is a Swiss knife for service integration. He offers a compelling look at how reducing integration overhead lowers maintenance costs and accelerates development cycles for rapid functionality growth.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quickly_debug_and_profile_mysql_databases"&gt;Quickly debug and profile MySQL databases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-trace-jdbc-statements-performance-in-mysql/"&gt;How to trace JDBC Statements performance in MySQL&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Francesco shows us how to quickly debug and profile SQL statements with Java applications using the MySQL JDBC driver. Using the WildFly CLI Francesco adds a JDBC connection to MySQL database in a container in a few simple steps. After creating the datasource he demonstrates the &lt;code&gt;hibernate.show_sql&lt;/code&gt; option in the application’s &lt;code&gt;persistence.xml&lt;/code&gt;. Once that’s done you can find all sorts of useful information about SQL statements in your logs. You’re all set to debug and tune!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_youtube_videos"&gt;YouTube videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;From unmissable demos to brilliant chat about the latest Java trends, the JBoss community has some great video content for you:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/Y2En5miRKjY"&gt;Build &amp;#38; Deploy WildFly Quickstarts on OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/2gQO4_7Z5CI"&gt;Securing a WildFly application with OpenID Connect on OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/22wg8oO9xXM"&gt;Quarkus Insights #86: Vaadin &amp;#38; Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/saTxdw-SPGA"&gt;Quarkus Insights #85: What’s new in Quarkus Kafka, REX with Loïc Mathieu&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/W2QPxfEU_bw"&gt;Build your first Java Serverless Function using Quarkus Quick start&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_see_you_next_time"&gt;See you next time&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;em&gt;Hope you enjoyed this edition. Please join us again in two weeks for our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Don Naro</dc:creator></entry><entry><title type="html">How to trace JDBC Statements performance in MySQL</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-trace-jdbc-statements-performance-in-mysql/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-trace-jdbc-statements-performance-in-mysql/</id><updated>2022-04-06T14:30:07Z</updated><content type="html">In this quick article we will learn an handy option which is available in MySQL JDBC Driver to debug and profile SQL statements run by Java applications. There are several strategies to trace SQL Statements that your application is running. For example, JPA applications typically use this option to show the actual SQL Statement that ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>An introduction to Linux bridging commands and features</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/06/introduction-linux-bridging-commands-and-features" /><author><name>Hangbin Liu</name></author><id>a0865fd8-d51d-482c-9d6a-131ded31de60</id><updated>2022-04-06T07:00:00Z</updated><published>2022-04-06T07:00:00Z</published><summary type="html">&lt;p&gt;A &lt;a href="/topics/linux"&gt;Linux&lt;/a&gt; bridge is a kernel module that behaves like a network switch, forwarding packets between interfaces that are connected to it. It's usually used for forwarding packets on routers, on gateways, or between VMs and network namespaces on a host.&lt;/p&gt; &lt;p&gt;The Linux bridge has included basic support for the &lt;a href="https://www.inap.com/blog/spanning-tree-protocol-explained/"&gt;Spanning Tree Protocol&lt;/a&gt; (STP), multicast, and &lt;a href="https://www.netfilter.org"&gt;Netfilter&lt;/a&gt; since the 2.4 and 2.6 kernel series. Features that have been added in more recent releases include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Configuration via &lt;a href="https://man7.org/linux/man-pages/man7/netlink.7.html"&gt;Netlink&lt;/a&gt;&lt;/li&gt; &lt;li&gt;VLAN filter&lt;/li&gt; &lt;li&gt;VxLAN tunnel mapping&lt;/li&gt; &lt;li&gt;Internet Group Management Protocol version 3 (IGMPv3) and Multicast Listener Discovery version 2 (MLDv2)&lt;/li&gt; &lt;li&gt;Switchdev&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In this article, you'll get an introduction to these features and some useful commands to enable and control them. You'll also briefly examine &lt;a href="https://www.openvswitch.org"&gt;Open vSwitch&lt;/a&gt; as an alternative to Linux bridging.&lt;/p&gt; &lt;h2&gt;Basic bridge commands&lt;/h2&gt; &lt;p&gt;All the commands used in this article are part of the &lt;code&gt;iproute2&lt;/code&gt; module, which invokes Netlink messages to configure the bridge. There are two &lt;code&gt;iproute2&lt;/code&gt; commands for setting and configuring bridges: &lt;code&gt;ip link&lt;/code&gt; and &lt;code&gt;bridge&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ip link&lt;/code&gt; can add and remove bridges and set their options. &lt;code&gt;bridge&lt;/code&gt; displays and manipulates bridges on final distribution boards (FDBs), main distribution boards (MDBs), and virtual local area networks (VLANs).&lt;/p&gt; &lt;p&gt;The listings that follow demonstrate some basic uses for the two commands. Both require administrator privileges, and therefore the listings are shown with the &lt;code&gt;#&lt;/code&gt; root prompt instead of a regular user prompt.&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Show help information about the &lt;code&gt;bridge&lt;/code&gt; object:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link help bridge # bridge -h&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create a bridge named &lt;code&gt;br0&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link add br0 type bridge&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Show bridge details:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip -d link show br0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Show bridge details in a pretty JSON format (which is a good way to get bridge key-value pairs):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip -j -p -d link show br0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add interfaces to a bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set veth0 master br0 # ip link set tap0 master br0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Spanning Tree Protocol&lt;/h2&gt; &lt;p&gt;The purpose of STP is to prevent a networking loop, which can lead to a traffic storm in the network. Figure 1 shows such a loop.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_1.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_1.png?itok=J-oXObCl" width="473" height="229" alt="Without STP, a network can be configured in a loop." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Without STP, a network can be configured in a loop. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;With STP enabled, the bridges will send each other Bridge Protocol Data Units (BPDUs) so they can elect a root bridge and block an interface, making the network topology loop-free (Figure 2).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_2.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_2.png?itok=ZJbkJjVE" width="471" height="229" alt="STP can choose a link and block it." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: STP can choose a link and block it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Linux bridging has supported STP since the 2.4 and 2.6 kernel series. To enable STP on a bridge, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge stp_state 1&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The Linux bridge does not support the Rapid Spanning Tree Protocol (RSTP).&lt;/p&gt; &lt;p&gt;Now you can show the STP blocking state on the bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip -j -p -d link show br0 | grep root_port "root_port": 1, # ip -j -p -d link show br1 | grep root_port "root_port": 0, # bridge link show 7: veth0@veth1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 master br0 state forwarding priority 32 cost 2 8: veth1@veth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 master br1 state forwarding priority 32 cost 2 9: veth2@veth3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 master br0 state blocking priority 32 cost 2 10: veth3@veth2: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 master br1 state forwarding priority 32 cost 2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The line labeled 9 in the output shows that the &lt;code&gt;veth2&lt;/code&gt; interface is in a blocking state, as illustrated in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_3.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_3.png?itok=RcE8tX77" width="470" height="257" alt="The link from br0 to veth2 is blocked." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The link from br0 to veth2 is blocked. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To change the STP hello time, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge hello_time 300 # ip -j -p -d link show br0 | grep \"hello_time\" "hello_time": 300,&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can use the same basic approach to change other STP parameters, such as maximum age, forward delay, ageing time, and so on.&lt;/p&gt; &lt;h2&gt;VLAN filter&lt;/h2&gt; &lt;p&gt;The VLAN filter was introduced in Linux kernel 3.8. Previously, to separate VLAN traffic on the bridge, the administrator needed to create multiple bridge/VLAN interfaces. As illustrated in Figure 4, three bridges—&lt;code&gt;br0&lt;/code&gt;, &lt;code&gt;br2&lt;/code&gt;, and &lt;code&gt;br3&lt;/code&gt;—would be needed to support three VLANs to make sure that VLAN traffic went to the corresponding VLANs.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_4.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_4.png?itok=gQOf45H2" width="874" height="328" alt="Without VLAN filter, three VLANs required three bridges and network configurations." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Without VLAN filter, three VLANs required three bridges and network configurations. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Without the VLAN filter, three VLANs required three bridges and network configurations.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But with the VLAN filter, just one bridge device is enough to set all the VLAN configurations, as illustrated in Figure 5.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_5.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_5.png?itok=W0KYDfZM" width="558" height="301" alt="With VLAN filter, a single bridge can serve multiple VLANs." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: With VLAN filter, a single bridge can serve multiple VLANs. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: With the VLAN filter, a single bridge can serve multiple VLANs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The following commands enable the VLAN filter and configure three VLANs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge vlan_filtering 1 # ip link set eth1 master br0 # ip link set eth1 up # ip link set br0 up # bridge vlan add dev veth1 vid 2 # bridge vlan add dev veth2 vid 2 pvid untagged # bridge vlan add dev veth3 vid 3 pvid untagged master # bridge vlan add dev eth1 vid 2-3 # bridge vlan show port vlan-id eth1 1 PVID Egress Untagged 2 3 br0 1 PVID Egress Untagged veth1 1 Egress Untagged 2 veth2 1 Egress Untagged 2 PVID Egress Untagged veth3 1 Egress Untagged 3 PVID Egress Untagged&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then the following command enables a VLAN filter on the &lt;code&gt;br0&lt;/code&gt; bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge vlan_filtering 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This next command makes the &lt;code&gt;veth1&lt;/code&gt; bridge port transmit only VLAN 2 data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev veth1 vid 2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following command, similar to the previous one, makes the &lt;code&gt;veth2&lt;/code&gt; bridge port transmit VLAN 2 data. The &lt;code&gt;pvid&lt;/code&gt; parameter causes untagged frames to be assigned to this VLAN at ingress (&lt;code&gt;veth2&lt;/code&gt; to bridge), and the &lt;code&gt;untagged&lt;/code&gt; parameter causes the packet to be untagged on egress (bridge to &lt;code&gt;veth2&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev veth2 vid 2 pvid untagged&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next command carries out the same operation as the previous one, this time on &lt;code&gt;veth3&lt;/code&gt;. The &lt;code&gt;master&lt;/code&gt; parameter indicates that the link setting is configured on the software bridge. However, because &lt;code&gt;master&lt;/code&gt; is a default option, this command has the same effect as the previous one:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev veth3 vid 3 pvid untagged master&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following command enables VLAN 2 and VLAN 3 traffic on &lt;code&gt;eth1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev eth1 vid 2-3&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To show the VLAN traffic state, enable VLAN statistics (added in kernel 4.7) as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge vlan_stats_enabled 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The previous command enables just global VLAN statistics on the bridge, and is not fine grained enough to show each VLAN's state. To enable per-VLAN statistics when there are &lt;code&gt;no&lt;/code&gt; port VLANs in the bridge, you also need to enable &lt;code&gt;vlan_stats_per_port&lt;/code&gt; (added in kernel 4.20). You can run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge vlan_stats_per_port 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then you can show per-VLAN statistics like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge -s vlan show port vlan-id br0 1 PVID Egress Untagged RX: 248 bytes 3 packets TX: 333 bytes 1 packets eth1 1 PVID Egress Untagged RX: 333 bytes 1 packets TX: 248 bytes 3 packets 2 RX: 0 bytes 0 packets TX: 56 bytes 1 packets 3 RX: 0 bytes 0 packets TX: 224 bytes 7 packets veth1 1 Egress Untagged RX: 0 bytes 0 packets TX: 581 bytes 4 packets 2 PVID Egress Untagged RX: 6356 bytes 77 packets TX: 6412 bytes 78 packets veth2 1 Egress Untagged RX: 0 bytes 0 packets TX: 581 bytes 4 packets 2 PVID Egress Untagged RX: 6412 bytes 78 packets TX: 6356 bytes 77 packets veth3 1 Egress Untagged RX: 0 bytes 0 packets TX: 581 bytes 4 packets 3 PVID Egress Untagged RX: 224 bytes 7 packets TX: 0 bytes 0 packets&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;VLAN tunnel mapping&lt;/h2&gt; &lt;p&gt;VxLAN builds Layer 2 virtual networks on top of a Layer 3 underlay. A VxLAN tunnel endpoint (VTEP) originates and terminates VxLAN tunnels. VxLAN bridging is the function provided by VTEPs to terminate VxLAN tunnels and map the VxLAN network identifier (VNI) to the traditional end host's VLAN.&lt;/p&gt; &lt;p&gt;Previously, to achieve VLAN tunnel mapping, administrators needed to add local ports and VxLAN network devices (netdevs) into a VLAN filtering bridge. The local ports were configured as trunk ports carrying all VLANs. A VxLAN netdev for each VNI would then need to be added to the bridge. VLAN to VNI mapping was achieved by configuring a port VLAN identifier (pvid) for each VLAN as on the corresponding VxLAN netdev, as shown in Figure 6.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_6.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_6.png?itok=cbHrDl-b" width="398" height="234" alt="VxLAN used to require multiple netdevs." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; VxLAN used to require multiple netdevs. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6. VxLAN used to require multiple netdevs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since 4.11, the kernel has provided a native way to support VxLAN bridging. The topology for this looks like Figure 7. The &lt;code&gt;vxlan0&lt;/code&gt; endpoint in this figure was added with lightweight tunnel (LWT) support to handle multiple VNIs.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_7.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_7.png?itok=8fJ6xtLr" width="399" height="236" alt="Now Linux bridging handle multiple VNIs with one VxLAN." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Now Linux bridging handle multiple VNIs with one VxLAN. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Now Linux bridging can handle multiple VNIs with one VxLAN.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To create a tunnel, you must first add related VIDs to the interfaces:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev eth1 vid 100-101 bridge vlan add dev eth1 vid 200 bridge vlan add dev vxlan0 vid 100-101 bridge vlan add dev vxlan0 vid 200&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now enable a VLAN tunnel mapping on a bridge port:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set dev vxlan0 type bridge_slave vlan_tunnel on&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can enable the tunnel with this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge link set dev vxlan0 vlan_tunnel on&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then add VLAN tunnel mapping:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge vlan add dev vxlan0 vid 2000 tunnel_info id 2000 # bridge vlan add dev vxlan0 vid 1000-1001 tunnel_info id 1000-1001 # bridge -j -p vlan tunnelshow [ { "ifname": "vxlan0", "tunnels": [ { "vlan": 100, "vlanEnd": 101, "tunid": 100, "tunidEnd": 101 },{ "vlan": 200, "tunid": 200 } ] } ]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Multicast&lt;/h2&gt; &lt;p&gt;Linux bridging has included support for IGMPv2 and MLDv1 support since kernel version 2.6. IGMPv3/MLDv2 support was added in kernel 5.10.&lt;/p&gt; &lt;p&gt;To use multicast, enable bridge multicast snooping, querier, and statistics as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge mcast_snooping 1 # ip link set br0 type bridge mcast_querier 1 # ip link set br0 type bridge mcast_stats_enabled 1 # tcpdump -i br0 -nn -l 02:47:03.417331 IP 0.0.0.0 &amp;gt; 224.0.0.1: igmp query v2 02:47:03.417340 IP6 fe80::3454:82ff:feb9:d7b4 &amp;gt; ff02::1: HBH ICMP6, multicast listener querymax resp delay: 10000 addr: ::, length 24&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, when snooping is enabled, the bridge uses IGMPv2/MLDv1. You can change the versions with these commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge mcast_igmp_version 3 ip link set br0 type bridge mcast_mld_version 2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After a port joins a group, you can show the multicast database (mdb) like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge mdb show dev br0 port br0 grp ff02::fb temp dev br0 port eth1 grp ff02::fb temp dev br0 port eth2 grp ff02::fb temp dev br0 port eth2 grp 224.1.1.1 temp dev br0 port br0 grp ff02::6a temp dev br0 port eth1 grp ff02::6a temp dev br0 port eth2 grp ff02::6a temp dev br0 port br0 grp ff02::1:ffe2:de9f temp dev br0 port eth1 grp ff02::1:ffe2:de9f temp dev br0 port eth2 grp ff02::1:ffe2:de9f temp&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bridging also supports multicast snooping and querier on a single VLAN. Set them as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan set vid 10 dev eth1 mcast_snooping 1 mcast_querier 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can show bridge xstats (multicast RX/TX information) with this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link xstats type bridge br0 IGMP queries: RX: v1 0 v2 1 v3 0 TX: v1 0 v2 131880 v3 0 IGMP reports: RX: v1 0 v2 1 v3 0 TX: v1 0 v2 496 v3 18956 IGMP leaves: RX: 0 TX: 0 IGMP parse errors: 0 MLD queries: RX: v1 1 v2 0 TX: v1 51327 v2 0 MLD reports: RX: v1 66 v2 6 TX: v1 3264 v2 213794 MLD leaves: RX: 0 TX: 0 MLD parse errors: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are other multicast parameters you can configure, including &lt;code&gt;mcast_router&lt;/code&gt;, &lt;code&gt;mcast_query_interval&lt;/code&gt;, and &lt;code&gt;mcast_hash_max&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Bridge switchdev&lt;/h2&gt; &lt;p&gt;Linux bridging is always used when virtual machines (VMs) connect to physical networks, by using the virtio tap driver. You can also attach a Single Root I/O Virtualization (SR-IOV) virtual function (VF) in a VM guest to get better performance (Figure 8).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_8.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_8.png?itok=s21JwYjI" width="596" height="291" alt="VFs in virtual machines." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: VFs in virtual machines. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But the way Linux used to deal with SR-IOV embedded switches limited their expressiveness and flexibility. And the kernel model for controlling the SR-IOV eSwitch did not allow any forwarding unless it was based on MAC/VLAN.&lt;/p&gt; &lt;p&gt;To make VFs also support dynamic FDB (as in Figure 9) and maintain the benefits of the VLAN filter while still providing optimal performance, Linux bridging added switchdev support in kernel version 4.9. Switchdev allows the offloading of Layer 2 forwarding to a hardware switch such as &lt;a href="https://github.com/Mellanox/mlxsw/wiki"&gt;Mellanox Spectrum devices&lt;/a&gt;, DSA-based switches, and MLX5 CX6 Dx cards.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/br_9.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/br_9.png?itok=_yu2G2C5" width="582" height="285" alt="Switchdev provides widespread support for offloading traffic to hardware." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Switchdev provides widespread support for offloading traffic to hardware. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In switchdev mode, the bridge is up and its related configuration is enabled, e.g., MLX5_BRIDGE for an MLX5 SRIOV eSwitch. Once in switchdev mode, you can connect the VF's representors to the bridge, and frames that are supposed to be transmitted by the bridge are transmitted by hardware only. Their routing will be done in the switch at the network interface controller (NIC).&lt;/p&gt; &lt;p&gt;Once a frame passes through the VF to its representor, the bridge learns that the source MAC of the VF is behind a particular port. The bridge adds an entry with the MAC address and port to its FDB. Immediately afterward, the bridge sends a message to the mlx5 driver, and the driver adds a relevant rule or line to two tables located in the eSwitch on the NIC. Later, frames with the same destination MAC address that come from the VF don't go through the kernel; instead, they go directly through the NIC to the appropriate port.&lt;/p&gt; &lt;p&gt;Switchdev support for embedded switches in NICs is simple, but for full-featured switches such as Mellanox Spectrum, the offloading capabilities are much richer, with support for link aggregation group (LAG) hashing (team, bonding), tunneling (VxLAN, etc.), routing, and TC offloading. Routing and TC offloading are out of scope for bridging, but LAGs can be attached to the bridge as well as to VxLAN tunnels, with full support for offloading.&lt;/p&gt; &lt;h2&gt;Bridging with Netfilter&lt;/h2&gt; &lt;p&gt;By default, the traffic forwarded by the bridge does not go through an iptables firewall. To let the iptables forward rules filter Layer 2 traffic, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge nf_call_iptables 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same procedure works for ip6tables and arptables.&lt;/p&gt; &lt;h2&gt;Bridge ageing time&lt;/h2&gt; &lt;p&gt;Ageing determines the number of seconds a MAC address is kept in the FDB after a packet has been received from that address. After this time has passed, entries are cleaned up. To change the timer, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge ageing_time 20000&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Bridging versus Open vSwitch&lt;/h2&gt; &lt;p&gt;Linux bridging is very useful and has become popular over the past few years. It supplies Layer 2 forwarding, and connects VMs and networks with VLAN/multicast support. Bridging on Linux is stable, reliable, and easy to set up and configure.&lt;/p&gt; &lt;p&gt;On the other hand, Linux bridging also has some limitations. It's missing some types of tunnel support, for instance. If you want to get easier network management, more tunnel support (GRE, VXLAN, etc.), Layer 3 forwarding, and integration with software-defined networking (SDN), you can try &lt;a href="https://www.openvswitch.org"&gt;Open vSwitch&lt;/a&gt; (OVS).&lt;/p&gt; &lt;p&gt;To learn more about Linux network interfaces and other networking topics, check out these articles from Red Hat Developer:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels"&gt;An introduction to Linux virtual interfaces: Tunnels&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking"&gt;Introduction to Linux interfaces for virtual networking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/blog/2021/04/01/get-started-with-xdp"&gt;Get started with XDP&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="/articles/2022/04/06/introduction-linux-bridging-commands-and-features" title="An introduction to Linux bridging commands and features"&gt;An introduction to Linux bridging commands and features&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Hangbin Liu</dc:creator><dc:date>2022-04-06T07:00:00Z</dc:date></entry><entry><title type="html">Turning Microservices Inside-Out</title><link rel="alternate" href="http://www.ofbizian.com/2022/04/turning-microservices-inside-out.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/04/turning-microservices-inside-out.html</id><updated>2022-04-05T08:42:00Z</updated><content type="html">There is a fantastic talk by Martin Kleppmann called “”. Once watched, it can change your perspective on databases and event logs irreversibly. While I agree with the outlined limitations of databases and the benefits of event logs, I’m not convinced of the practicality of replacing databases with event logs. I believe the same design principles used for turning databases inside-out, should instead be applied at a higher, service design level to ensure microservices stream changes from inside-out. With that twist, within the services, we can keep using traditional databases for what they are best for - efficiently working with mutable state and also use event logs to reliably propagate changes among services. With the help of frameworks such as Debezium which can act as a connecting tissue between databases and event logs, we can benefit from the time-tested and familiar database technology and modern event logs such as Red Hat’s managed at the same time. This inside-out mindset requires a deliberate focus on offering outbound APIs in microservices to stream all relevant state change and domain events from within the service to the outside world. This merge of microservices movement with the event driven emerging trends is what I call turning the microservices data inside-out. MICROSERVICES API TYPES To build up this idea, I will look into microservices from the point of different API types they provide and consume. A common way to describe microservices is as independently deployed components, built around a business domain, that own their data and are exposed over APIs. That is very similar to how databases are described in the post mentioned above - a black box with a single API that goes in and out. Data flowing from microservices’ inbound to outbound APIs I believe a better way to think about microservices would be one where every microservice is composed of inbound and outbound APIs where the data flows through and a meta API that describes these APIs. While inbound APIs are well known today, outbound APIs are not used as much, and the responsibilities of meta API are spread around various tools and proliferating microservices technologies. To make the inside-out approach work, we need to make outbound and meta APIs first-class microservices constructs and improve the tooling and practices around these areas. INBOUND APIS Inbound APIs are what every microservice has today in the form of service endpoints. These APIs are outside-in, and they allow outside systems to interact with the service directly through commands and queries or indirectly through events. Inbound APIs are the norm in microservices today In terms of implementation, these are typically REST-based APIs that offer mutating or read-only operations for synchronous operations, fronted by a load balancing gateway. These can also be implemented as queues for asynchronous command-based interactions, or topics for event-based interactions. The responsibilities and governance of these APIs are well understood and they form the majority of the microservices API landscape today. OUTBOUND APIS What I refer to as outbound APIs here are the interactions that originate from within the service and go to outside services and systems. The majority of these are queries and commands initiated by the service and targeted to dependent services owned by somebody else. What I also put under this category are the outbound events that originate from within the service. Outbound events are different from the query and commands targeted for a particular endpoint because an outbound event is defined by the service without concrete knowledge of the existing and possible future recipients. Regardless of the indirect nature of the API, there is still the expectation that these events are generated predictably and reliably for any significant change that happens within the service (typically caused by inbound interactions). Today, outbound events are often an afterthought. They are either created for the needs of a specific consumer that depends on them, or they are added later in the service lifecycle, not by the service owners but other teams responsible for data replication. On both occasions, the possible use cases of outbound events remain low and diminish its potential. The challenging part with outbound events is implementing a uniform and reliable notification mechanism for any change that happens within the service. To apply this approach uniformly in every microservice and for any kind of database, the tools here have to be non-intrusive and developer-friendly. Not having good frameworks that support this pattern, not having proven patterns, practices, and standards are impediments preventing the adoption of outbound events as a common top-level microservices construct. Outbound events implemented through change data capture To implement outbound events, you can include the logic of updating a database and publishing an event to a messaging system in your application code but that leads to the well-known . Or you could try to replace the traditional database with an event log, or use specialized event sourcing platforms. But if you consider that your most valuable resources in a project are the people and their proven tools and practices, replacing a fundamental component such as the database with something different will have a significant impact. A better approach would be to keep using the relational databases and all the surrounding tools and practices that have served fine for decades and complement your database with a connecting tissue such as Debezium (disclaimer: I’m the product manager for Debezium at Red Hat and I’m biased about it). I believe the best implementation approach for outbound events is the which uses a single transaction to both perform the normal database update dictated by the service logic and insert a message into a specific outbox table within the same database. Once the transaction is written to the database’s transaction log, Debezium picks up the outbox message from the log and sends it to Apache Kafka. This has nice properties such as "read your own writes" semantics, where a subsequent query to the service returns the newly persisted record and at the same time, we get reliable, asynchronous, propagation of changes via Apache Kafka. Debezium can selectively capture changes from the database transaction logs, transform and publish them into Kafka in a uniform way acting as an outbound eventing interface of the services. Debezium can be embedded into the Java application runtimes as a library, or decoupled as a . It is a plug-and-play component you add to your service regardless of whether it is a legacy service or created from scratch. It is the missing configuration-based outbound eventing API for any service. META APIS Today meta APIs describe the inbound and outbound APIs, enabling their governance, discovery, and consumption. They are implemented in siloed tools around a specific technology. In my definition, an OpenAPI definition for a REST endpoint published to an API portal is an example of meta API. An AsyncAPI definition for a messaging topic that is published to a schema registry is an example of meta API too. The schema change topic that Debezium publishes database schema change events (which are different from the data change events) is an example of meta API. There are various capabilities in other tools that describe the data structures and the APIs serving them that can all be classified as meta APIs. So in my definition, meta APIs are all the artifacts that allow different stakeholders to work with the service and enable other systems to use the inbound and outbound APIs. The evolving responsibilities of Meta APIs One of the fundamental design principles of microservices is to make them independently updatable and deployable. But today there are still significant amounts of coordination required among service owners for upgrades that involve API changes. Service owners need better meta API tools to subscribe for updates from dependent services and prepare to change timely. The meta API tools need to be integrated deeper into development and operational activities to increase agility. Meta API tools today are siloed, passive, and disparate across the technology stack. Instead, meta tools need to reflect the changing nature of service interactions towards an event-driven approach and play a more proactive role in automating some of the routine tasks of the development and operational teams. EMERGING TRENDS THE RISE OF OUTBOUND EVENTS Outbound events are already present as the preferred integration method for most modern platforms. Most cloud services emit events. Many data sources (such as Cockroach changefeeds, MongoDB change streams) and even file systems (for example Ceph ) can emit state change events. Custom-built microservices are not an exception here. Emitting state change or domain events is the most natural way for modern microservices to fit uniformly among the event-driven systems they are connected to in order to benefit from the same tooling and practices. Outbound events are bound to become a top-level microservices design construct for many reasons. Designing services with outbound events can help replicate data during an process. Outbound events are also the enabler for implementing elegant inter-service interactions through the Outbox Patterns and complex business transactions that span multiple services using a non-blocking implementation. Outbound events fit nicely into the architecture where a service is designed with its data consumers in mind. Data mesh claims that for data to fuel innovation, its ownership must be federated among domain data owners who are accountable for providing their data as products… In short, rather than having a centralized data engineering team to replicate data from every microservice through an ETL process, it is better if microservices are owned jointly with developers and data engineers and design the services to make the data available in the first place. What better way to do that than outbound events with real-time data streaming through Debezium, Apache Kafka, and Schema Registry. To sum up, outbound events align microservices with the Unix philosophy where “the output of every program becomes the input of a yet unknown program”. To future proof your services, you have to design them in a way to let the data flow from inbound to outbound APIs. This allows all the services to be developed and operated uniformly using modern event-oriented tools and patterns, and unlocks yet unknown future uses of data exposed through events. CONVERGENCE OF META API TOOLS With the increasing adoption of event-driven architectures and faster pace of service evolution, the responsibilities and the importance of meta APIs are growing too. The scope of meta API tools is no longer limited to but includes asynchronous APIs too. The meta APIs are expanding towards enabling faster development cycles by ensuring safe schema evolution through compatibility checks, notifications for updates, code generation for bindings, test simulations, and so forth. As a consumer of a service, I want to discover existing endpoints and data formats, the API compatibility rules, limits, and SLAs the service complies with in one place. At the same time, I want to get notifications for any changes that are coming, any deprecations, updates to the APIs, or any new APIs the service is going to offer that might be of interest to me. Not only that, developers are challenged to ship code faster and faster, and modern API tools can automate the process of schema and event structure discovery. Once a schema is discovered and added to the registry, a developer can quickly generate code bindings for their language and start developing in an IDE. Then, other tools could use the meta API definitions and generate tests and mocks, and simulate load by emitting dummy events with something like or even . At runtime, the contextual information available in the meta APIs can enable the platforms I’m running the application on to inject the , register it with monitoring tools, and so on. Overall, the role of meta API is evolving towards playing a more active role in the asynchronous interaction ecosystem by automating some of the coordination activities among service owners, increasing developer productivity, and automating operations teams’ tasks. And for that to become a reality, the different tools containing API metadata, code generation, test stimulation, environment management must converge, standardize and integrate better. STANDARDIZATION OF THE EVENT-DRIVEN SPACE While event-driven architecture (EDA) has a long history, recent drivers such as cloud adoption, microservices architecture, and a faster pace of change have amplified the relevance and adoption of EDA. Similar to the consolidation and the standardization that happens with Kubernetes and its ecosystem on the platform space, there is a consolidation and community-driven standardization that is happening in the event-driven space around Apache Kafka. Let see a few concrete examples. Apache Kafka has reached the point of becoming the de facto standard platform for event streaming, the same way AWS S3 is for object store, and Kubernetes is for container orchestration. Kafka has a huge community behind, a large open source ecosystem of tools and services, and possibly the largest adoption as eventing infrastructure by modern digital organizations. There are all kinds of self-hosted Kafka offerings, managed services by boutique companies, cloud providers, and recently by Red Hat too (Red Hat OpenShift Streams for Apache Kafka is a managed I’m involved with and I’d love to hear your feedback). Kafka as an API for log-based messaging is so widespread that even non-Kafka projects such as Pulsar, Red Panda, Azure Event Hubs offer compatibility with it. Kafka today is more than a 3rd party architectural dependency. Kafka influences how services are designed and implemented, it dictates how systems are scaled and made highly available, it drives how the users consume the data in real-time. But Kafka alone is like a bare Kubernetes platform without any pods. Let’s see what else in the Kafka ecosystem is a must-have complement and is becoming a de facto standard too. A Schema Registry is as important for asynchronous APIs as an API manager is for synchronous APIs. In many streaming scenarios, the event payload contains structured data that both the producer and consumer need to understand and validate. A schema registry provides a central repository and a common governance framework for schema documents and enables applications to adhere to these contracts. Today there are registries such as Apicurio by Red Hat, by Aiven, registries by Cloudera, , Confluent, Azure, AWS, and more. While schema repositories are increasing in popularity and consolidating in the capabilities and practices around schema management, at the same time they vary in licensing restrictions. Not only that, schema registries tend to leak into client applications in the form of Kafka Serializer/Deserializer (SerDes), converters, and other client dependencies. So the need for an open and vendor-neutral standard where the implementations can be swapped has been apparent for a while. And the good news is that Schema Registry API exists in CNCF and few registries such and have already started to follow it. Complementing the open source Kafka API with an open source service registry API and common governance practices feels right and I expect the adoption and consolidation in this space to grow to make the whole meta API concept a cornerstone of event-driven architectures. Similar to EDA, the concept of Change Data Capture () is not new. But the recent drivers around event-driven systems and the increasing demand for access to real-time data are building the momentum for transaction-log-driven event streaming tools. Today, there are many closed source, point-and-click tools (such as Striim, HVR, Qlik) that rely on the same transaction log concept to replicate data point-to-point. There are cloud services such as AWS DMS, Oracle GoldenGate Cloud Service and Google Datastream that will stream your data into their services (but never in the opposite direction). There are many databases, key-value stores that stream changes too. The need for an open source and vendor-neutral CDC standard that different vendors can follow and downstream change-event consumers can rely on is growing. To succeed, such a standard has to be managed on a vendor-neutral foundation and be part of a larger related ecosystem. The closest thing that exists today is CNCF which is already home to AsyncAPI, , Schema Registry, and specifications too. Today, by far, the leading open source project in the CDC space is Debezium. Debezium is by major companies, embedded into cloud services from Google, Heroku, Confluent, Aiven, Red Hat, embedded into multiple open source projects, and used by many proprietary solutions that we won’t ever know about. If you are looking for a standard in this domain, the closest de facto standard is Debezium. To clarify, with a CDC standard I don’t mean an API for data sources to emit changes. I mean standard conventions for data sources and connecting tissues such as Debezium to follow when converting database transaction logs into events. That includes data mapping (from database field types into JSON/Avro types), data structures (for example Debezium’s Before/After message structure), , partitioning of tables into topics, and primary keys into topic partitions, transaction indicators, and so forth. If you are going heavy on CDC, using Debezium will ensure consistent semantics for mapping from database transaction log entries into Apache Kafka events that are uniform across datasources.  Specifications and implementation around the Apache Kafka ecosystem There are already a few existing specifications from the event-driven space at CNCF that are gaining traction. * is OpenAPI’s equivalent for event-driven applications that recently joined CNCF. It offers a specification to document your event-driven systems to maintain consistency, and governance across different teams and tools. * CloudEvents (also part of CNCF) aims to eliminate the metadata challenge by specifying mandatory metadata information into what could be called a standard envelope. It also offers libraries for multiple programming languages for multiple protocols, which streamlines interoperability. * (another CNCF sandbox project) standardizes the creation and management of trace information that reveals the end-to-end path of events through multiple applications. * CNCF Serverless Workflow is a vendor-neutral spec for coordination asynchronous stateless and stateful interaction. * The service registry proposal in CNCF we discussed above... Whether we call it standardization, community adoption or something else, we cannot deny the consolidation process around event-driven constructs and the rise of some open source projects as de facto standards. SUMMARY Microservices are focused around the encapsulation of data that belongs to a business domain and exposing it over a minimal API as possible. But that is changing. Data going out of a service is as important as data going into it. Exposing data in microservices can no longer be an afterthought. Siloed and inaccessible data wrapped in a highly decoupled microservice are of limited value. There are new users of data and possible yet unknown users that will demand access to discoverable, understandable, real-time data. To satisfy the needs of these users, microservices have to turn data inside-out and be designed with outbound APIs that can emit data and meta APIs that make the consumption of data a self-service activity. Projects such as Apache Kafka, Debezium and schema registries are a natural enabler of this architecture and with the help of the various open source asynchronous specifications are turning into de facto choice for implementing future-proof event-driven microservices.  This article was originally published on InfoQ .</content><dc:creator>Unknown</dc:creator></entry></feed>
