<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">DevOpsDays Raleigh 2022 - Talking Architecture Shop (slides)</title><link rel="alternate" href="http://www.schabell.org/2022/04/devopsdays-raleigh-2022-talking-architecture-shop-slides.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/04/devopsdays-raleigh-2022-talking-architecture-shop-slides.html</id><updated>2022-04-13T18:00:00Z</updated><content type="html">I've  that I had a talk accepted to the DevOpsDays Raleigh 2022 conference this year.  Today was the day after travel to Raleigh that we got  to chat with a room of super enthusiastic architects. Thanks for the time and lending us your ears. Below you'll find the talk title and abstract along with the slides for your viewing pleasure. My session is from a series called Talking Architecture Shop. This is focusing on architecture research for solutions in the DevOps domain that scale and will be co-presented with my good friend . TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE DEVOPS AT SCALE  You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise DevOps implementations that scale? This session takes attendees on a tour of multiple use cases covering DevOps challenges with hybrid cloud management with GitOps, DevOps in healthcare, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop!</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title>Manage namespaces in multitenant clusters with Argo CD, Kustomize, and Helm</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/13/manage-namespaces-multitenant-clusters-argo-cd-kustomize-and-helm" /><author><name>Saumeya Katyal</name></author><id>b6f7bcb3-322c-4f97-a993-ee3073d528a3</id><updated>2022-04-13T07:00:00Z</updated><published>2022-04-13T07:00:00Z</published><summary type="html">&lt;p&gt;Cluster administrators on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; need to create namespaces for multiple developer teams and limit their use of resources by provisioning those namespaces with resource quotas and limit ranges. In this article, you'll learn how to automate these tasks with &lt;a href="https://argo-cd.readthedocs.io/en/stable/"&gt;Argo CD&lt;/a&gt;, and how to use either &lt;a href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt; or &lt;a href="https://helm.sh/docs/"&gt;Helm charts&lt;/a&gt; to simplify the process. The result implements the fundamentals of &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;, whereby any changes to the repository update the deployed resources.&lt;/p&gt; &lt;p&gt;There are two types of configuration that administrators have to deal with are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/#:~:text=A%20resource%20quota%2C%20defined%20by,by%20resources%20in%20that%20namespace"&gt;Resource quota&lt;/a&gt;: A Kubernetes object that controls the amount of CPU or memory consumed by a namespace. This quota can also limit the number of resources that can be created in a namespace.&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/policy/limit-range/#:~:text=A%20LimitRange%20is%20a%20policy,per%20PersistentVolumeClaim%20in%20a%20namespace."&gt;Limit range&lt;/a&gt;: This is used in Kubernetes along with resource quotas. Although resource quotas control the overall resource consumption of an entire namespace, they do not place any limit on a pod or container within that namespace, so a single pod or container could use up all of the namespace's resources. Limit ranges specify resources available per pod or container.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;While the general principles outlined here apply to any Kubernetes environment, some of the examples in this article assume that you are deploying on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and can use its graphical user interface as well as the &lt;code&gt;oc&lt;/code&gt; command-line interface (CLI).&lt;/p&gt; &lt;h2&gt;A simple Argo CD application&lt;/h2&gt; &lt;p&gt;To automate the administrator's configuration tasks, you'll use &lt;a href="https://argo-cd.readthedocs.io/en/stable/"&gt;Argo CD&lt;/a&gt;, a powerful &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous delivery&lt;/a&gt; tool for Kubernetes resources. Because namespaces and quotas are Kubernetes resources, Argo CD can manage them.&lt;/p&gt; &lt;p&gt;In this section, you'll create a simple Argo CD application from &lt;a href="https://github.com/saumeya/blog-example-repo"&gt;an example in my GitHub repository&lt;/a&gt;. In the sections that follow this one, I'll demonstrate two better ways to create the application using Kustomize and Helm along with Argo CD. But we'll start with a simple example to help you get your bearings with Argo CD.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a Git repository on your system based on the &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/namespaces-config"&gt;namespaces-config example in my GitHub repository&lt;/a&gt;. The repository comprises the manifest for all namespaces, quotas, and limit ranges for all the teams in the example.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add &lt;a href="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/04-syncwaves-hooks.html"&gt;Syncwaves&lt;/a&gt; under the &lt;code&gt;annotations&lt;/code&gt; property in these configuration files. Syncwaves is valuable for imposing an order on separate activities. In this case, you need to create a namespace before you can associate a resource quota and limit range to it. The following files show how you can assure that the namespace is created first by assigning a Syncwave of -1, whereas the resource quota and limit range have a Syncwave of 0.&lt;/p&gt; &lt;p&gt;The configuration for a namespace looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: Namespace metadata: name: dev annotations: argocd.argoproj.io/sync-wave: "-1" labels: argocd.argoproj.io/managed-by: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration for a resource quota looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ResourceQuota metadata: name: resource-quota namespace: dev annotations: argocd.argoproj.io/sync-wave: "0" spec: hard: pods: "10"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration for a limit range looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: LimitRange metadata: name: limits namespace: dev annotations: argocd.argoproj.io/sync-wave: "0" spec: limits: - default: cpu: 200m memory: 512Mi defaultRequest: cpu: 100m memory: 256Mi type: Container&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a id="common-step" name="common-step"&gt;&lt;/a&gt;Log in to your OpenShift cluster as a cluster administrator.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The Argo CD application controller needs additional permissions to create resource quotas and limit ranges in cluster namespaces. Use &lt;a href="https://docs.openshift.com/container-platform/4.9/authentication/using-rbac.html"&gt;OpenShift cluster roles and cluster role bindings&lt;/a&gt; to grant these permissions to the application controller of the default &lt;code&gt;argocd&lt;/code&gt; instance in the &lt;code&gt;openshift-gitops&lt;/code&gt; namespace. To carry out this step, create a &lt;code&gt;ClusterRole&lt;/code&gt; and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; and apply them to your cluster.&lt;/p&gt; &lt;p&gt;The configuration for a cluster role looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # "namespace" omitted since ClusterRoles are not namespaced name: quota-limit-cluster-role rules: - apiGroups: [""] #specifies core api groups resources: ["resourcequotas", "limitranges"] verbs: ["create"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command to create the &lt;code&gt;ClusterRole&lt;/code&gt; on the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f &lt;cluster-role-file-name&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration for a cluster role binding looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows Service Account to create resource quotas and limit ranges in any namespace. kind: ClusterRoleBinding metadata: name: create-quota-limit-global roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: quota-limit-cluster-role # Name of cluster role to be referenced subjects: - kind: ServiceAccount name: openshift-gitops-argocd-application-controller namespace: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command to create the &lt;code&gt;ClusterRoleBinding&lt;/code&gt; on the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f cluster-role-binding.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create an Argo CD Application via the user interface (Figure 1), with the following sample Git repository:&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/app-create.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/app-create.png?itok=PrXZOAAE" width="1409" height="796" alt="In the OpenShift console, you can create an Argo CD Application from this article's example repository." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. In the OpenShift console, you can create an Argo CD Application from this article's example repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: In the OpenShift console, you can create an Argo CD Application from this article's example repository.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You could also use an &lt;code&gt;Application&lt;/code&gt; custom resource to create an application using the CLI. If you want to take this route, your configuration should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: namespace-management spec: destination: name: '' namespace: default server: 'https://kubernetes.default.svc' source: path: namespaces-config repoURL: 'https://github.com/saumeya/blog-example-repo.git' targetRevision: HEAD project: default&lt;/code&gt;&lt;/pre&gt; Enter the following command to create the application: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f &lt;application-cr-file-name&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Refresh and synchronize the application to create the namespaces with their related quotas and limit ranges (Figure 2).&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/simple-ns-app.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/simple-ns-app.png?itok=tvMIIbdx" width="880" height="581" alt="The Topology view of the OpenShift console shows the Argo CD Application and its limit assignments to other namespaces." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Topology view of the OpenShift console shows the Argo CD Application and its limit assignments to other namespaces. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Topology view of the OpenShift console shows the Argo CD application and its limit assignments to other namespaces.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Argo CD automates the assignment of resource limits, but the procedure shown in this example so far requires you to maintain an individual manifest for each team. This repetition of files makes it tedious to manage the configurations, especially if you're dealing with a large number of teams and projects. In the following sections, you'll see some better approaches that make use of Kustomize and Helm.&lt;/p&gt; &lt;h2&gt;Use Kustomize to create and manage resources&lt;/h2&gt; &lt;p&gt;The procedure in this section optimizes some of the tasks in the previous section and leaves others unchanged. By using custom patches in &lt;a href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt;, a Kubernetes-native configuration management tool, you can avoid creating multiple manifests and reuse common elements from resource quotas and limit ranges.&lt;/p&gt; &lt;p&gt;I have placed the relevant files for this example in my &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/kustomize-namespace-config/teams"&gt;GitHub repository&lt;/a&gt;. The &lt;code&gt;teams&lt;/code&gt; directory defines all the patches for different teams in the base manifests. The associated resources are created when you create the Argo CD Application.&lt;/p&gt; &lt;p&gt;Here is the &lt;code&gt;kustomization.yaml&lt;/code&gt; for a patch:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: team-a bases: - ../../base patches: - target: kind: Namespace name: default-dev patch: |- - op: replace path: /metadata/name value: team-a - target: kind: ResourceQuota name: resource-quota patch: |- - op: replace path: /metadata/name value: quota-team-a - op: replace path: /spec/hard/limits.cpu value: 1 - op: replace path: /spec/hard/services value: 10 - target: kind: LimitRange name: limit-range patch: |- - op: replace path: /metadata/name value: quota-team-a&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;kustomization.yaml&lt;/code&gt; file for all the teams:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;bases: - ./team-a - ./team-b - ./team-c&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now follow &lt;a href="#common-step"&gt;steps 3, 4, and 5 from the previous example&lt;/a&gt; to grant additional permissions and create Argo CD Applications to manage your namespaces (Figure 3). Make sure to correctly specify the &lt;code&gt;Path&lt;/code&gt; in step 5 to &lt;code&gt;kustomize-namespace-config/teams&lt;/code&gt;. Refresh and synchronize the application.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kustomize-ns.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kustomize-ns.png?itok=8Kpm2WL6" width="715" height="576" alt="The Topology view of the OpenShift console shows the Argo CD Application created by Kustomize and its limit assignments to other namespaces." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. The Topology view of the OpenShift console shows the Argo CD Application created by Kustomize and its limit assignments to other namespaces. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The Topology view of the OpenShift console shows the Argo CD Application created by Kustomize and its limit assignments to other namespaces.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Use Helm and an ApplicationSet to create and manage resources&lt;/h2&gt; &lt;p&gt;&lt;a href="https://helm.sh/docs/"&gt;Helm templates&lt;/a&gt; can also be used to parameterize configurations for namespaces, resource quotas, and limit ranges. A simple use of Helm charts, however, would require you to create more Argo CD applications to manage these namespaces. To avoid creating multiple applications one by one, use an &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/application-set/"&gt;ApplicationSet&lt;/a&gt; resource to specify the value files and create all the applications in one go.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a Helm chart and add the resource files from &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/helm-namespace-config"&gt;my example repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here is the &lt;code&gt;templates/namespace.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: Namespace metadata: name: {{ .Values.namespace }} annotations: argocd.argoproj.io/sync-wave: "-1" labels: argocd.argoproj.io/managed-by: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;templates/limit-range.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: LimitRange metadata: name: limit-range namespace: {{ .Values.namespace }} annotations: argocd.argoproj.io/sync-wave: "0" spec: limits: - default: cpu: {{ .Values.limits.default.cpu | default "200m" }} memory: {{ .Values.limits.default.memory | default "512Mi" }} defaultRequest: cpu: {{ .Values.limits.defaultRequest.cpu | default "100m"}} memory: {{ .Values.limits.defaultRequest.memory | default "256Mi"}} type: {{ .Values.limits.type | default "Container" }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;templates/resource-quota.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ResourceQuota metadata: name: resource-quota namespace: {{ .Values.namespace }} annotations: argocd.argoproj.io/sync-wave: "0" spec: hard: requests.cpu: {{ .Values.quota.requests.cpu | default "1"}} requests.memory: {{ .Values.quota.requests.memory | default "1Gi"}} limits.cpu: {{ .Values.quota.limits.cpu | default "2"}} limits.memory: {{ .Values.quota.limits.memory | default "2Gi"}} pods: {{ .Values.quota.pods | default "10"}} persistentvolumeclaims: {{ .Values.quota.persistentvolumeclaims | default "20"}} resourcequotas: {{ .Values.quota.resourcequotas | default "1"}} services: {{ .Values.quota.services | default "5"}}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;values.yaml&lt;/code&gt; file contains default values. You can create value files with different names and specify those in Argo CD while creating an application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# Default values for namespace-app. # This is a YAML-formatted file. # Declare variables to be passed into your templates. namespace: default-dev #specifies the quota to be used for resources quota: requests: cpu: '1' memory: 1Gi limits: cpu: '2' memory: 2Gi pods: "10" persistentvolumeclaims: "20" resourcequotas: "1" services: "5" #specifies the limit ranges for the chart limits: default: memory: 512Mi defaultRequest: cpu: 100m memory: 256Mi type: Container &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is the &lt;code&gt;Chart.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v2 name: helm-namespace-config description: A Helm chart for Namespace Management type: application version: 0.1.0 appVersion: "1.16.0"&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Follow &lt;a href="#common-step"&gt;steps 3 and 4&lt;/a&gt; from the first example in this article to grant additional permissions and create Argo CD applications to manage your namespaces.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Now use an &lt;code&gt;ApplicationSet&lt;/code&gt; resource to create multiple applications. Check the &lt;a href="https://github.com/saumeya/blog-example-repo/tree/main/helm-namespace-config/teams"&gt;teams directory&lt;/a&gt; in my example repository, which contains custom value files for various teams. Add the relative paths of these custom value files in the list generator. This procedure generates the applications for specific teams with the required configurations, as illustrated in Figure 4.&lt;/p&gt; &lt;p&gt;Here is the configuration file for the &lt;code&gt;ApplicationSet&lt;/code&gt; resource:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: namespace-config namespace: openshift-gitops spec: generators: - list: elements: - filepath: teams/team-a.yaml name: team-a - filepath: teams/team-b.yaml name: team-b - filepath: teams/team-c.yaml name: team-c template: metadata: name: '{{name}}-namespace-config' spec: project: default syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true source: repoURL: 'https://github.com/saumeya/blog-example-repo' targetRevision: HEAD path: helm-namespace-config helm: valueFiles: - '{{filepath}}' destination: server: 'https://kubernetes.default.svc' namespace: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Apply the &lt;code&gt;ApplicationSet&lt;/code&gt; configuration file to your cluster by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f &lt;application-set-file-name&gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/appset-helm.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/appset-helm.png?itok=Kyu4SeVW" width="1440" height="399" alt="The Application Set created three Applications." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. The Application Set created three Applications. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The ApplicationSet has created three applications.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can now synchronize each application and click on an individual application to see the created resources, as illustrated in Figure 5.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/team-a-guide.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/team-a-guide.png?itok=bMATIYJJ" width="853" height="316" alt="The Topology view of the OpenShift console shows the Team A application created by Helm." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. The Topology view of the OpenShift console shows the Team A application created by Helm. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: The Topology view of the OpenShift console shows the Team A application created by Helm.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;More information about how to create applications using Helm can be found in &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/helm/"&gt;Argo CD's documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Conclusion&lt;/h3&gt; &lt;p&gt;This article has shown how to use Argo CD in conjunction with other convenient open source tools to simplify the creation, management, and configuration of Kubernetes namespaces. Whenever you need to change the quotas or limits, all you need to do is modify the configuration files in the source repository and Argo CD does the rest of the work.&lt;/p&gt; &lt;p&gt;If you want to learn more about using Argo CD with Red Hat OpenShift, check out &lt;a href="https://developers.redhat.com/blog/2020/10/01/building-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-1"&gt;Part 1&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/2020/10/14/building-modern-ci-cd-workflows-for-serverless-applications-with-red-hat-openshift-pipelines-and-argo-cd-part-2"&gt;Part 2&lt;/a&gt; of the "Building modern CI/CD workflows for serverless applications with Red Hat OpenShift Pipelines and Argo CD" series on Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/13/manage-namespaces-multitenant-clusters-argo-cd-kustomize-and-helm" title="Manage namespaces in multitenant clusters with Argo CD, Kustomize, and Helm"&gt;Manage namespaces in multitenant clusters with Argo CD, Kustomize, and Helm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Saumeya Katyal</dc:creator><dc:date>2022-04-13T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 4.2.7 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-2-7" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-2-7</id><updated>2022-04-13T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.2.7 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">Using the default Servlet in WildFly applications</title><link rel="alternate" href="http://www.mastertheboss.com/web/jboss-web-server/using-the-default-servlet-in-wildfly-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/web/jboss-web-server/using-the-default-servlet-in-wildfly-applications/</id><updated>2022-04-12T14:19:06Z</updated><content type="html">This article will teach you how to use the default Servlet available in Undertow Web Server for some concerns, such as directory listing or static resources serving. Most Web servers have a default Servlet available. What’s the use case for a default Servlet? You can use it mainly for two reasons: To serve static resources ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Observability in 2022: Why it matters and how OpenTelemetry can help</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help" /><author><name>Ben Evans</name></author><id>058dd256-91a7-4914-b9eb-7cc4fd8a2040</id><updated>2022-04-12T07:00:00Z</updated><published>2022-04-12T07:00:00Z</published><summary type="html">&lt;div class="paragraph"&gt; &lt;p&gt;This article explains the basics of &lt;em&gt;observability&lt;/em&gt; for developers. We’ll look at why observability should interest you, its current level of maturity, and what to look out for to make the most of its potential.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Two years ago, James Governor of the developer analyst firm Redmonk remarked, "Observability is making the transition from being a niche concern to becoming a new frontier for user experience, systems, and service management in web companies and enterprises alike." Today, observability is hitting the mainstream.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As 2022 gets underway, you should expect to hear more about observability as a concern to take seriously. However, lots of developers are still unsure about what observability actually is—​and some of the descriptions of the subject can be vague and imprecise. Read on to get a foundation in this emerging topic.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_what_is_observability"&gt;What is observability?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The discipline of observability grew out of several separate strands of development, including application performance monitoring (APM) and the need to make orchestrated systems such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; more comprehensible. Observability aims to provide highly granular insights into the behavior of systems, along with rich context.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Overall, implementing observability is conceptually fairly simple. To enable observability in your projects, you should:&lt;/p&gt; &lt;/div&gt; &lt;div class="olist arabic"&gt; &lt;ol class="arabic"&gt;&lt;li&gt; &lt;p&gt;Instrument systems and applications to collect relevant data (e.g. metrics, traces, and logs).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Send this data to a separate external system that can store and analyze it.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide visualizations and insights into systems as a whole (including query capability for end users).&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The final step—​the query and visualization capabilities—​are key to the power of observability.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The theoretical background for the approach comes from system control theory—​essentially asking, "How well can the internal state of a system be inferred from outside?"&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This requirement has been set down as a constraint in a &lt;a href="https://www.honeycomb.io/blog/so-you-want-to-build-an-observability-tool/"&gt;blog posting by Charity Majors&lt;/a&gt;: "Observability requires that you not have to predefine the questions you will need to ask, or optimize those questions in advance." Meeting this constraint requires the collection of sufficient data to accurately model the system’s internal state.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Incident resolution is a really good fit for observability, and it’s where the practice originated. Site reliability experts (SREs) and &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; teams typically focus on incident response. They are interested in a holistic understanding of complex behavior that replaces fragmentary or pre-judged views based on just one or two pieces of the overall system.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;But if the right data is being collected, the stakeholders for observability are much broader than just SREs, production support, and DevOps folks. Observability gives rise to different goals depending on the stakeholder group using the data.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The promise of a wider use for observability can be seen in some of the discussions about whether observability systems should also collect business-relevant metrics, costs, etc. Such data provides additional context and possible use cases for an observability system, but some practitioners argue that it dilutes the goal of the system.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_why_is_observability_important"&gt;Why is observability important?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As applications increasingly move to the cloud, they are becoming more complex. There are typically more services and components in modern applications, with more complex topology as well as a much faster pace of change (driven by practices such as &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The growing complexity of applications parallels the increasing popularity of technologies with genuinely new behaviors that were created for the cloud. The highlights here include &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; environments, dynamically scaling services (especially Kubernetes), and Function-as-a-Service deployments such as AWS Lambda.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This new world makes root cause analysis and incident resolution potentially a lot harder, yet the same questions still need answering:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;What is the overall health of my solution?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What is the root cause of errors and defects?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What are the performance bottlenecks?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which of these problems could impact the user experience?&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability is therefore at the heart of architecting robust, reliable systems for the cloud. The search for an architectural solution to these questions is perhaps best expressed in a &lt;a href="https://copyconstruct.medium.com/monitoring-and-observability-8417d1952e1c"&gt;posting by Cindy Sridharan&lt;/a&gt;: "Since it’s still not possible to predict every single failure mode a system could potentially run into or predict every possible way in which a system could misbehave, it becomes important that we build systems that can be debugged armed with evidence and not conjecture."&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This new cloud-native world is especially important to Red Hat users because so much of it is based on open source and open standards. The core runtimes, instrumentation components, and telemetry are all open source, and observability components are managed through industry bodies such as the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_what_data_do_we_need_to_collect"&gt;What data do we need to collect?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability data is often conceptualized in terms of three pillars:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Distributed traces&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Metrics and monitoring&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Logs&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Although some have questioned the value of this categorization, it is a relatively simple mental model, and so is quite useful for developers who are new to observability. Let’s discuss each pillar in turn.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_distributed_traces"&gt;Distributed traces&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A &lt;em&gt;distributed trace&lt;/em&gt; is a record of a single service invocation, usually corresponding to a single request from an individual user. The trace includes the following metadata about each request:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Which instance was called&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which containers they were running on&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Which method was invoked&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;How the request performed&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;What the results were&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In distributed architectures, a single service invocation typically triggers downstream calls to other services. These calls, which contribute to the overall trace, are known as &lt;em&gt;spans&lt;/em&gt;, so a trace forms a tree structure of spans. Each span has associated metadata.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The span perspective corresponds to the &lt;em&gt;extrinsic&lt;/em&gt; view of service calls in traditional monitoring. Distributed traces are used to instrument service calls that are request-response oriented. There are additional difficulties associated with calls that do not fit this pattern that tracing struggles to account for.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_metrics_and_monitoring"&gt;Metrics and monitoring&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;em&gt;Metrics&lt;/em&gt; are numbers measuring specific activity over a time interval. A metric is typically encoded as a tuple consisting of a timestamp, name, value, and dimensions. The dimensions are a set of key-value pairs that describe additional metadata about the metric. Furthermore, it should be possible for a data storage engine to aggregate values across the dimensions of a particular metric to create a meaningful result.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There are many examples of metrics across many different aspects of a software system:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;System metrics (including CPU, memory, and disk usage)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Infrastructure metrics (e.g., from AWS CloudWatch)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Application metrics (such as APM or error tracking)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;User and web tracking scripts (e.g., from Google Analytics)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Business metrics (e.g., customer sign-ups)&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Metrics can be gathered from all of the different levels on which the application operates, from very low-level operating system counters all the way up to human and business-scale metrics. Unlike logs or traces, the data volume of metrics does not scale linearly with request traffic—​the exact relationship varies based on the type of metric being collected.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_logs"&gt;Logs&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;em&gt;Logs&lt;/em&gt; constitute the third pillar of observability. These are defined as immutable records of discrete events that happen over time. Depending on the implementation, there are basically three types of logs: plain text, structured, and binary format. Not all observability products support all three.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Examples of logs include:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;System and server logs (syslog)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Firewall and network system logs&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Application logs (e.g., Log4j)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Platform and server logs (e.g., Apache, NGINX, databases)&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_observability_tools_open_source_offerings_on_the_rise"&gt;Observability tools: Open source offerings on the rise&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The APM/monitoring market segment used to be dominated by proprietary vendors. In response, various &lt;a href="https://developers.redhat.com/topics/open-source"&gt;free and open source&lt;/a&gt; software projects started or were spun out of tech companies. Early examples include &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; for metrics, and &lt;a href="https://zipkin.io/"&gt;Zipkin&lt;/a&gt; and &lt;a href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt; for tracing. In the logging space, the "ELK stack" (&lt;a href="https://www.elastic.co/"&gt;Elasticsearch&lt;/a&gt;, &lt;a href="https://www.elastic.co/logstash/"&gt;Logstash&lt;/a&gt;, and &lt;a href="https://www.elastic.co/kibana/"&gt;Kibana&lt;/a&gt;) gained market share and became popular.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As software continues to become more complex, more and more resources are required to provide a credible set of instrumentation components. For proprietary observability products, this trend creates duplication and inefficiency. The market has hit an inflection point, and it is becoming more efficient for competing companies to collaborate on an open source core and compete on features further up the stack (as well as on pricing).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This historical pattern is not an unusual dynamic for open source, and has shown up as a switch from proprietary to open source driven offerings as this market segment migrates from APM to observability. The move to open source can also be partly attributed to the influence of observability startups that have been fully or partially open source since their inception.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;One key milestone was the merger of the OpenTracing and OpenCensus projects to form &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt;, a major project within CNCF. The project is still maturing, but is gaining momentum. An increasing number of users are investigating and implementing OpenTelemetry, and this number seems set to grow significantly during 2022. A recent survey from the CNCF showed that 49 percent of users were already using OpenTelemetry, and that number is rising rapidly.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_the_state_of_opentelemetry"&gt;The state of OpenTelemetry&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Some developers are still confused by exactly what OpenTelemetry actually is. The project offers a set of standards, formats, client libraries, and associated software components. The standards are explicitly cross-platform and not tied to any particular technology stack.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;OpenTelemetry provides a framework that integrates with open source and commercial products and can collect observability data from apps written in many languages. Because the implementations are open source, they are at varying levels of technical maturity, depending on the interest that OpenTelemetry has attracted in specific language communities.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;From the Red Hat perspective, the &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java/JVM&lt;/a&gt; implementation is particularly relevant, being one of the most mature implementations. Components in other major languages and frameworks, such as &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;, are also fairly mature.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The implementations work with applications running on bare metal or virtual machines, but OpenTelemetry overall is definitely a cloud-first technology.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;It’s also important to recognize what OpenTelemetry is &lt;em&gt;not.&lt;/em&gt; It isn’t a data ingest, storage, backend, or visualization component. Such components must be provided either by other open source projects or by vendors to produce a full observability solution.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_observability_vs_apm"&gt;Observability vs. APM&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You may wonder if observability is just a new and flashier name for application performance monitoring. In fact, observability has a number of advantages for the end user over traditional APM:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Vastly reduced vendor lock-in&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open specification wire protocols&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open source client components&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Standardized architecture patterns&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Increasing quantity and quality of open source backend components&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In the next two years, you should expect to see a further strengthening of key open source observability projects, as well as market consolidation onto a few segment leaders.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_java_observability"&gt;Java observability&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Java is hugely important to Red Hat’s users, and there are particular challenges to the adoption of observability in Java stacks. Fundamentally, Java technology was designed for a world where JVMs ran on bare metal in data centers, so the applications don’t easily map to tools designed for containerization.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The world is changing, however. Cloud-native deployments—​especially containers—​are here and being adopted quickly, albeit at varying rates across different parts of the industry.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The traditional Java application lifecycle consists of a number of phases: bootstrap, intense class loading, warmup (with JIT compilation), and finally a long-lived steady state (lasting for days or weeks) with relatively little class loading or JIT. This model is challenged by cloud deployments, where containers might live for much shorter time periods and cluster sizes might be dynamically readjusted.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As it moves to containers, Java has to ensure that it remains competitive along several key axes, including footprint, density, and startup time. Fortunately, ongoing research and development within OpenJDK is trying to make sure that the platform continues to optimize for these characteristics—​and Red Hat is a key contributor to this work.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you’re a Java developer looking to adapt to this new world, the first thing to do is prepare your application or company plan for observability. OpenTelemetry is likely to be the library of choice for many developers for both tracing and metrics. Existing libraries, especially &lt;a href="https://micrometer.io/"&gt;Micrometer&lt;/a&gt;, are also likely to have a prominent place in the landscape. In fact, interoperability with existing components within the Java ecosystem is a key goal for OpenTelemetry.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_status_and_roadmap"&gt;Status and roadmap&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;OpenTelemetry has several subprojects that are at different levels of maturity.&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;The Distributed Tracing specification is at v1.0 and is being widely deployed into production systems. It replaces OpenTracing completely, and the OpenTracing project has officially been &lt;a href="https://medium.com/opentracing/opentracing-has-been-archived-fb2848cfef67"&gt;archived&lt;/a&gt;. The Jaeger project, one of the most popular distributing tracing backends, has also discontinued its client libraries and will default to OpenTelemetry protocols going forward.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The OpenTelemetry Metrics project is not quite as advanced, but it is approaching v1.0 and General Availability (GA). At time of writing, the protocol is at the Stable stage and the API is at Feature Freeze. It is anticipated that the project might reach v1.0/GA during April 2022.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Finally, the Logging specification is still in Draft stage and is not expected to reach v1.0 until late 2022. There is still a certain acknowledged amount of work to do on the spec, and participation is actively being sought by the working groups.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Overall, OpenTelemetry as a whole will be considered to be v1.0/GA when the Metrics standard reaches v1.0 alongside Tracing.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The major takeaways are that observability is reaching more and more developers and is noticeably gathering steam. Some analysts even anticipate that OpenTelemetry formats will become the largest single contributor to observability traffic as early as 2023.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_red_hat_and_the_opentelemetry_collector"&gt;Red Hat and the OpenTelemetry Collector&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As the industry starts to embrace OpenTelemetry, it’s important to help users decide what to do with all the telemetry data that is being generated. One key piece is the &lt;a href="https://opentelemetry.io/docs/collector/"&gt;OpenTelemetry Collector&lt;/a&gt;. This component runs as a network service that can receive, proxy, and transform data. It enables users to keep data and process it internally, or forward it to a third party.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Collector helps solve one of the major hurdles to adoption faced by a new standard such as OpenTelemetry: Dealing with legacy applications and with infrastructure that already exists. The Collector can understand and speak to many different legacy protocols and payloads, and can translate them into OpenTelemetry-supported protocols. In turn, these open protocols can be consumed by a vast number of vendors who embrace the specification. This unification of older protocols is a major shift in the observability space, and is going to offer a level of flexibility that we haven’t seen before.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Red Hat is deeply committed to the OpenTelemetry community and has released the OpenTelemetry Collector as a component of the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; Container Platform, branded as &lt;a href="https://catalog.redhat.com/software/operators/detail/5ec54a5c78e79e6a879fa271"&gt;OpenShift distributed tracing data collection&lt;/a&gt;. This helps our users take advantage of all the capabilities the OpenTelemetry Collector has to offer in order to provide a better, more open approach to observability.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The core architectural capabilities of the Collector can be summarized as follows:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;&lt;strong&gt;Usability:&lt;/strong&gt; A reasonable default configuration that works out of the box&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Performant under varying loads and configurations&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Observability:&lt;/strong&gt; A good example of an observable service&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensibility:&lt;/strong&gt; Customizable without touching the core code&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unification:&lt;/strong&gt; A single codebase that supports traces, metrics, and logs&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability is an emerging set of ideas and DevOps practices that help handle the complexity of modern architectures and applications, rather than any specific set of products.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Observability absorbs and extends classic monitoring systems, and helps teams identify the root cause of issues. More broadly, it allows stakeholders to answer questions about their application and business, including forecasting and predictions about what could go wrong.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A diverse collection of tools and technologies are in use, which leads to a large matrix of possible deployments. This has architectural consequences, so teams need to understand how to set up their observability systems in a way that works for them.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;One key technology is OpenTelemetry. It is rapidly gaining popularity, but is still maturing, and the open source groups and standards need more participation, especially by end users.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help" title="Observability in 2022: Why it matters and how OpenTelemetry can help"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ben Evans</dc:creator><dc:date>2022-04-12T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.8.0.Final released - New REST layer by default, GraalVM 22.0 and much more!</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-8-0-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-8-0-final-released/</id><updated>2022-04-12T00:00:00Z</updated><content type="html">2.8.0.Final comes with a lot of refinements and new features: Move Assertj outside of our BOM New REST layer by default GraalVM 22.0 Support for OIDC Proof Of Key for Code Exchange (PKCE) QuarkusTransaction API Elasticsearch Dev Services And much more! Migration Guide To migrate from 2.7, please refer to...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">Rise of J2Cl: Java web development after GWT￼</title><link rel="alternate" href="https://blog.kie.org/2022/04/rise-of-j2cl-java-web-development-after-gwt.html" /><author><name>Dmitrii Tikhomirov</name></author><id>https://blog.kie.org/2022/04/rise-of-j2cl-java-web-development-after-gwt.html</id><updated>2022-04-11T18:08:55Z</updated><content type="html">It looks like 15 years of GWT are coming to the end, and besides that web development has dramatically changed since 2006. There is now no chaos of conflicting browser implementations that require to run multiple permutations each. At the same time modern web development frameworks are far from ideal. For instance, a very strong advantage of GWT was the ecosystem around Maven – the stability and usability of this solution was incredible, especially when big teams worked on large projects. Google, the main developer of GWT, left the project and started J2CL, the successor of GWT, which takes the very best practices to a new level. Their documentation calls it out as being used in many high performance projects such as Gmail, Inbox, Docs, Slides, and Calendar. Initially J2CL was developed to be used in the Bazel environment. After several years of hard work the community, led by Colin Alworth, released the first public J2CL version for Maven – j2cl-maven-plugin. So let’s take a look at what it is and how it works. J2CL AND CLOSURE COMPILER J2CL is responsible for only one task – to transpile a set of Java classes into a set of JavaScript files. Google’s Closure Compiler is responsible for merging this set of javascripts into one executable JS script, its optimization and minification. Closure Compiler is extremely efficient in minification and optimization of the JavaScript, it simply has no competitors. GENERATING OUR FIRST J2CL PROJECT Let’s start from a simple one module application. Luckily for us, we can generate it from a pre-build archetype. Download the archetype if you don’t have it:  mvn org.apache.maven.plugins:maven-dependency-plugin:get \ -DrepoUrl=https://repo.vertispan.com/j2cl/ \ -Dartifact=com.vertispan.j2cl.archetypes:j2cl-archetype-simple:0.19 Now we can generate a simple application: mvn archetype:generate -DarchetypeGroupId=com.vertispan.j2cl.archetypes \ -DarchetypeArtifactId=j2cl-archetype-simple \ -DarchetypeVersion=0.19 Let’s take a look at the result: ├── pom.xml └── src ├── main │ ├── java │ │ └── org │ │ └── treblereel │ │ └── j2cl │ │ ├── App.java │ │ └── App.native.js │ └── webapp │ ├── WEB-INF │ │ └── web.xml │ ├── css │ │ └── simpleapp.css │ └── index.html └── test └── java └── org └── treblereel └── j2cl └── AppTest.java – App.java is a starting point of our application and there’s is one point I have to highlight below. – App.native.js used to specify how to start our application to the Closure Compiler, because it knows nothing about it. Usage of native.js is a very large topic and a separate article can be written about it. – AppTest.java is just a J2CL-compatible unit test that runs in HtmlUnit, it’s also possible to use ChromeDriver to run it in a real browser but it takes longer. – pom.xml – here the only interesting part for us is the j2cl-maven-plugin section. For now it contains only the &lt;compilationLevel&gt; declaration used to set which level of optimization we are going to use during the compilation of the project. ADVANCED is the most efficient one, so Closure Compiler does aggressive renaming, dead code removal, global inlining and so on. But in some cases Closure Compiler needs our help and care – we have to declare which properties or methods should not be removed or renamed. BUNDLE is less strict and better suitable for development because each compilation round takes less time compared to ADVANCED.  RUNNING AND BUILDING OUR J2CL APPLICATION j2cl-maven-plugin allows us to run our application in the development mode with build-in hot code reload and source map debug. To start devmode, run the following command in the terminal: &gt; mvn j2cl:watch When the application started, run the following command in the second terminal: &gt; mvn jetty:run There is no need to run ‘mvn clean’ each time because J2CL will recompile everything from scratch, and we can reuse the results from the previous run. Moreover, there is an option to use global cache between several projects to reduce compilation time. To build ‘.war’ we should run ‘mvn package’, there is nothing new here, everything is pretty familiar to GWT developers. OK, WHAT IS NEW COMPARED TO GWT * GWT modules are gone, yes, no more modules. So J2CL will try to compile direct and transitive dependencies from pom.xml, that is why we should set ‘provided scope’ to annotation processors and shade them. * GWT.create gone as well * GWT generators are gone, now we should use APT-based generators. * What about GWT components and widgets we used before ? Most of them have been ported to J2CL. * Does @GwtIncompatible work? Yes, it is still here. AND WHAT IS THE VALUE OF IT FOR US? Right now we are focused on migration of our existing projects from gwt2 to j2cl. There are many libraries that have been migrated to j2cl and there are many libraries that support gwt2 and j2cl. I would like to highlight elemental2-* wrappers, DominoKit, Nalu, and many others. Gwt2 has been ported as gwtproject – set of migrated modules. Errai framework – the core component of our applications has been re-implemented as Crysknife project. In the upcoming posts i am going to address several topics: * Existing libraries and frameworks that are J2CL-compatible * Defines – how to propagate variables to J2CL * How we can improve generated code with native.js * Externs – why do we need them and how to write our own externs * Semi-reflection  * Interoperability with TS * And maybe many more The post appeared first on .</content><dc:creator>Dmitrii Tikhomirov</dc:creator></entry><entry><title>Introduction to the Node.js reference architecture, Part 8: TypeScript</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript" /><author><name>Dominic Harries</name></author><id>77b82ab1-239d-4d61-9ee6-f3ccffd31756</id><updated>2022-04-11T07:00:00Z</updated><published>2022-04-11T07:00:00Z</published><summary type="html">&lt;p&gt;One of the key choices you make when building an enterprise &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application is whether to use plain &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; or a dialect that supports type validation. While participating in the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/"&gt;Node.js reference architecture effort&lt;/a&gt;, we've pulled together many internal Red Hat and IBM teams to discuss our experience with using both plain JavaScript and &lt;a href="https://www.typescriptlang.org"&gt;TypeScript&lt;/a&gt;. Our projects seem to be split between the two, and it's often "love it or hate it" when using types with JavaScript.&lt;/p&gt; &lt;p&gt;TypeScript is in widespread use, particularly among enterprise developers coming from other languages such as &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. TypeScript was recently voted the third most loved programming language in StackOverflow's &lt;a href="https://insights.stackoverflow.com/survey/2021#most-loved-dreaded-and-wanted-language-love-dread"&gt;annual developer survey&lt;/a&gt;—far ahead of JavaScript itself.&lt;/p&gt; &lt;p&gt;This article covers why you might want to use TypeScript and how to get started, along with an introduction to the recommendations in the Node.js reference architecture. As with all our Node.js reference architecture recommendations, we focus on defining a set of good and reliable default choices. Some teams will deviate from the recommendations based on their assessment of what best fits their use case.&lt;/p&gt; &lt;p&gt;Read the series so far:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;Choosing web frameworks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;Code Coverage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Part 8: TypeScript&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why use TypeScript?&lt;/h2&gt; &lt;p&gt;JavaScript has come a long way from its humble beginnings as a lightweight scripting language inside the browser. Technologies such as Node.js have propelled it to become one of the leading languages for back-end development.&lt;/p&gt; &lt;p&gt;But as codebases grow in size, it can be increasingly difficult to track down errors and keep track of the data flowing through an application. That's true in any language, but it's a particular problem in weakly typed languages like JavaScript.&lt;/p&gt; &lt;p&gt;TypeScript is designed to address this problem. By adding type annotations to variables, TypeScript can help to document the data a program uses, catch errors, and give developers confidence that they can change code in one place without breaking other parts of their codebase.&lt;/p&gt; &lt;p&gt;Many code editors now have excellent TypeScript support. This support enables code completion, immediate feedback on type errors, powerful automatic refactoring, and other useful features. For instance, &lt;a href="https://code.visualstudio.com"&gt;Visual Studio Code&lt;/a&gt; is a widely used editor that comes with extensive &lt;a href="https://code.visualstudio.com/docs/languages/typescript"&gt;support for TypeScript&lt;/a&gt;. The TypeScript wiki contains a &lt;a href="https://github.com/Microsoft/TypeScript/wiki/TypeScript-Editor-Support"&gt;list of other editors&lt;/a&gt; with TypeScript support.&lt;/p&gt; &lt;p&gt;Most popular third-party JavaScript libraries now ship with TypeScript type definitions or make them available via the &lt;a href="https://github.com/DefinitelyTyped/DefinitelyTyped"&gt;Definitely Typed&lt;/a&gt; repository.&lt;/p&gt; &lt;p&gt;These capabilities have caused TypeScript to explode in popularity.&lt;/p&gt; &lt;h2&gt;Get started with TypeScript&lt;/h2&gt; &lt;p&gt;TypeScript has been designed to be easy to adopt, even for existing JavaScript projects. You can incrementally enable TypeScript a single file at a time while leaving the rest of your project in JavaScript.&lt;/p&gt; &lt;p&gt;To demonstrate this flexibility, we'll port a very simple Node.js application to TypeScript. The application consists of a single JavaScript file named &lt;code&gt;fill.js&lt;/code&gt; in the project's &lt;code&gt;src&lt;/code&gt; directory. The code fills an array with a value:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;function fillArray(len, val) { const arr = []; for (let i = 0; i &lt; len; i++) { arr.push(val); } return arr; } module.exports = { fillArray };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step one is to install a TypeScript compiler. Because Node.js does not natively understand TypeScript files, they must be compiled to JavaScript before they can be executed. The compilation from TypeScript to JavaScript is called &lt;em&gt;transpiling&lt;/em&gt;. There are multiple transpilers available (see the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/development/typescript"&gt;reference architecture&lt;/a&gt; for details), but we'll use the standard TypeScript compiler &lt;code&gt;tsc&lt;/code&gt;. Install it as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npm install --save-dev typescript&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're using any built-in Node.js modules, you also need the types for these:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npm install --save-dev @types/node&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The compilation process is configured using a &lt;code&gt;tsconfig.json&lt;/code&gt; file. This configuration controls all the parameters for TypeScript compilation. The Node.js community maintains a recommended configuration that you can install as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npm install --save-dev @tsconfig/node16&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are using a Node.js version older than 16, you can check &lt;a href="https://github.com/tsconfig/bases"&gt;the list of bases&lt;/a&gt; for recommended configurations compatible with older versions.&lt;/p&gt; &lt;p&gt;Add Node.js options to your &lt;code&gt;tsconfig.json&lt;/code&gt; file as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "extends": "@tsconfig/node16/tsconfig.json", "compilerOptions": { "allowJs": true, "strict": false, "outDir": "./build" }, "include": ["./src/**/*"] }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This configuration specifies that all files under the &lt;code&gt;src&lt;/code&gt; directory should be compiled and put in the build directory. It also allows your source files to stay written in JavaScript (these will be copied across to the build directory without modification) and disables strict mode (further details later on strict mode). There are many further options that you can set—please see our recommendations in the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/development/typescript"&gt;reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To run the compile, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;npx tsc&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this simple example, because we haven't defined any data types, the compiler created an identical &lt;code&gt;fill.js&lt;/code&gt; file in the build directory.&lt;/p&gt; &lt;h3&gt;Adding some TypeScript&lt;/h3&gt; &lt;p&gt;Node.js supports two module systems:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;CommonJS: The traditional format, which uses the &lt;code&gt;require&lt;/code&gt; keyword to import code and &lt;code&gt;module.exports&lt;/code&gt; to export it.&lt;/li&gt; &lt;li&gt;ES modules: A newer format using the &lt;code&gt;import&lt;/code&gt; keyword to import code and the &lt;code&gt;export&lt;/code&gt; keyword to export it. This format is supported by both Node.js and web browsers.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;TypeScript supports only the ES module format, so in addition to renaming your example file to &lt;code&gt;src/fill.ts&lt;/code&gt;, you need to update its export:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;export function fillArray(len, val) { const arr = []; for (let i = 0; i &lt; len; i++) { arr.push(val); } return arr; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This code now compiles successfully, even though you haven't added any types. This is because strict mode is set to false in the &lt;code&gt;tsconfig.json&lt;/code&gt; file. If you set the mode to &lt;code&gt;true&lt;/code&gt;, you will see an error like the following when you compile:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; src/fill.ts:1:27 - error TS7006: Parameter 'len' implicitly has an 'any' type. src/fill.ts:1:32 - error TS7006: Parameter 'val' implicitly has an 'any' type. &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You can add some annotations to the argument list in the first line to fix these errors:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;export function fillArray(len: number, val: any) { const arr = []; for (let i = 0; i &lt; len; i++) { arr.push(val); } return arr; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The changes make the compilation succeed. Even better, if you accidentally forget which way round the parameters go and call the method like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;console.log(fillArray("-", 5));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;TypeScript gives another helpful error:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;error TS2345: Argument of type 'string' is not assignable to parameter of type 'number'.&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;We recommend enabling strict mode for new projects, but when migrating existing projects it may be easier to leave the mode disabled.&lt;/p&gt; &lt;p&gt;Many editors can be configured to immediately show TypeScript errors rather than waiting until you run the compiler. Editors may also offer other advanced features such as code completion and automatic refactoring.&lt;/p&gt; &lt;h2&gt;Node.js reference architecture recommendations&lt;/h2&gt; &lt;p&gt;Teams need to make a number of key choices when using TypeScript. These include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Should transpilers be used? If so, which ones?&lt;/li&gt; &lt;li&gt;What should be shipped: the original files, or the transpiled versions?&lt;/li&gt; &lt;li&gt;What TypeScript options and configuration should be used?&lt;/li&gt; &lt;li&gt;How should types for npm packages be published?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The Node.js reference architecture contains further recommendations, including how to use TypeScript with tools such as &lt;code&gt;nodemon&lt;/code&gt; and best practices for deployment based on the experience our team has gained through deployments within Red Hat, IBM, and our customers.&lt;/p&gt; &lt;p&gt;Those recommendations are well defined in the Node.js reference architecture, so instead of repeating them here, we encourage you to head over to the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/development/typescript"&gt;TypeScript section of the reference architecture itself&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What's next?&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Node.js reference architecture series&lt;/a&gt;. While you wait for the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you'll see the work we've already done and the kinds of topics you can look forward to in the future.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="topics/nodejs"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript" title="Introduction to the Node.js reference architecture, Part 8: TypeScript"&gt;Introduction to the Node.js reference architecture, Part 8: TypeScript&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Dominic Harries</dc:creator><dc:date>2022-04-11T07:00:00Z</dc:date></entry><entry><title type="html">What’s new in Jakarta EE 10</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/whats-new-in-jakarta-ee-10/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/whats-new-in-jakarta-ee-10/</id><updated>2022-04-08T15:56:30Z</updated><content type="html">Jakarta EE 10 is the first major release of Jakarta EE since the “jakarta” namespace update. Many of the component specifications are introducing Major or Minor version updates that are going to reflect in the implementation APIs. Let’s learn in this article what we can expect from the upcoming new major release. Project status Firstly, ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>3 ways to install a database with Helm charts</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/07/3-ways-install-database-helm-charts" /><author><name>Wanja Pernath</name></author><id>f6ca6909-f9d0-4fc8-873e-9f17b651b575</id><updated>2022-04-07T07:00:00Z</updated><published>2022-04-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/helm/all"&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Helm uses a packaging format called &lt;em&gt;charts,&lt;/em&gt; which include all of the Kubernetes resources that are required to deploy an application, such as deployments, services, ingress, and so on. Helm charts are very useful for installing applications and performing upgrades on a Kubernetes cluster.&lt;/p&gt; &lt;p&gt;In chapter 3 of my e-book &lt;a href="https://developers.redhat.com/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;&lt;em&gt;Getting GitOps: A Practical Platform with OpenShift, Argo CD and Tekton&lt;/em&gt;&lt;/a&gt;, I discuss the basics of creating and using Helm charts. I also dig into the the use case of creating a post-install and post-upgrade job.&lt;/p&gt; &lt;p&gt;However, that chapter provided a very basic example that focused only on what's necessary to create and deploy a Helm chart. This article will demonstrate some more advanced techniques to create a chart that could be installed more than once in the same namespace. It also shows how you could easily install a dependent database with your chart.&lt;/p&gt; &lt;p&gt;The source code for this example can be found in the &lt;a href="https://github.com/wpernath/book-example/tree/main/better-helm"&gt;GitHub repository that accompanies my book&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The use case: How to install a dependent database with a Helm chart&lt;/h2&gt; &lt;p&gt;Chapter 1 of my book outlines the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;-based &lt;code&gt;person-service&lt;/code&gt;, a simple REST API service that reads and writes personal data from and into a PostgreSQL database. A Helm chart packages this service, and needs to provide all the dependencies necessary to successfully install it. As discussed in that chapter, you have three options to achieve that goal:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use the corresponding OpenShift template to install the necessary PostgreSQL database&lt;/li&gt; &lt;li&gt;Use the &lt;a href="https://github.com/CrunchyData/postgres-operator/"&gt;CrunchyData Postgres Operator&lt;/a&gt; (or any other Operator-defined PostgreSQL database extension) for the database&lt;/li&gt; &lt;li&gt;Install a dependent Helm chart, such as the &lt;a href="https://artifacthub.io/packages/helm/bitnami/postgresql"&gt;PostgreSQL chart by Bitnami&lt;/a&gt;, with your chart&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;No matter which route you take, however, you also need to ensure that your chart can be installed multiple times on each namespace. So let's tackle that task first.&lt;/p&gt; &lt;h2&gt;Make the chart installable multiple times in the same namespace&lt;/h2&gt; &lt;p&gt;The most crucial step for making your chart installable multiple times in the same namespace is to use generated names for all the manifest files. Therefore, you need an object called &lt;code&gt;Release&lt;/code&gt; with the following properties:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;Name&lt;/code&gt;: The name of the release&lt;/li&gt; &lt;li&gt;&lt;code&gt;Namespace&lt;/code&gt;: Where you are going to install the chart&lt;/li&gt; &lt;li&gt;&lt;code&gt;Revision&lt;/code&gt;: The revision number of this release (starts at 1 on install, and each update increments it by one)&lt;/li&gt; &lt;li&gt;&lt;code&gt;IsInstall&lt;/code&gt;: &lt;code&gt;true&lt;/code&gt; if it's an installation process&lt;/li&gt; &lt;li&gt;&lt;code&gt;IsUpgrade&lt;/code&gt;: &lt;code&gt;true&lt;/code&gt; if it's an upgrade process&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If you want to make sure that your chart installation won't conflict with any other installations in the same namespace, do the following:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-config labels: app.kubernetes.io/part-of: {{ .Release.Name }}-chart data: APP_GREETING: |- {{ .Values.config.greeting | default "Yeah, it's openshift time" }} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This creates a &lt;code&gt;ConfigMap&lt;/code&gt; with the name of the release, followed by a dash, followed by &lt;code&gt;config&lt;/code&gt;. Of course, you now need to make sure that the &lt;code&gt;ConfigMap&lt;/code&gt; is being read by the deployment accordingly:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;- image: "{{ .Values.deployment.image }}:{{ .Values.deployment.version }}" envFrom: - configMapRef: name: {{ .Release.Name }}-config [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are updating all the other manifest files in your Helm's &lt;code&gt;templates&lt;/code&gt; folder, you can install your chart multiple times.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm install person-service1 &lt;path to chart&gt; $ helm install person-service2 &lt;path to chart&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With that task out of the way, we can now consider each of the three potential approaches outlined above in turn.&lt;/p&gt; &lt;h2&gt;Install the database via an existing OpenShift template&lt;/h2&gt; &lt;p&gt;By far easiest way to install a PostgreSQL database in an OpenShift namespace is by using an OpenShift template. We did it several times in my book. The call is simple:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc new-app postgresql-persistent \ -p POSTGRESQL_USER=wanja \ -p POSTGRESQL_PASSWORD=wanja \ -p POSTGRESQL_DATABASE=wanjadb \ -p DATABASE_SERVICE_NAME=wanjaserver &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But how could you automate this process? There is no way to execute this call from within a Helm chart installation. (Well, you could do it by using a pre-install hook, but that would be quite ugly.)&lt;/p&gt; &lt;p&gt;Fortunately, the OpenShift client has a function called &lt;code&gt;process&lt;/code&gt; that processes a template. The result of this call is a list of YAML objects that can then be installed into OpenShift.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc process postgresql-persistent -n openshift -o yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're piping the result into a new file, you would get something like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: v1 kind: List items: - apiVersion: v1 kind: Secret metadata: labels: template: postgresql-persistent-template name: postgresql stringData: database-name: sampledb database-password: KSurRUMyFI2fiVpx database-user: user0U4 - apiVersion: v1 kind: Service [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're not happy with the default parameters for username, password, and database name, call the process function with the &lt;code&gt;-p PARAM=VALUE&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc process postgresql-persistent -n openshift -o yaml \ -p POSTGRESQL_USER=wanja \ -p POSTGRESQL_PASSWORD=wanja \ -p POSTGRESQL_DATABASE=wanjadb \ -p DATABASE_SERVICE_NAME=wanjaserver &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Place the resulting file into your chart's &lt;code&gt;templates&lt;/code&gt; folder, and it will be used to install the database. If you have a closer look at the file, you can see that it's using &lt;code&gt;DATABASE_SERVICE_NAME&lt;/code&gt; as manifest names for its &lt;code&gt;Service&lt;/code&gt;, &lt;code&gt;Secret&lt;/code&gt;, and &lt;code&gt;DeploymentConfig&lt;/code&gt; objects, which would make it impossible to install your resulting chart more than once into any namespace.&lt;/p&gt; &lt;p&gt;If you're providing the string &lt;code&gt;-p DATABASE_SERVICE_NAME='pg-{{ .Release.Name }}'&lt;/code&gt; instead of the fixed string &lt;code&gt;wanjaserver&lt;/code&gt;, then this will be used as the object name for these manifest files. However, if you try to install your Helm chart now, you'll get some verification error messages. This is because &lt;code&gt;oc process&lt;/code&gt; generates some top-level status fields that the Helm parser does not understand, so you need to remove them.&lt;/p&gt; &lt;p&gt;The only thing you now need to do is to connect your &lt;code&gt;person-service&lt;/code&gt; deployment with the corresponding database instance. Simply add the following entries to the &lt;code&gt;env&lt;/code&gt; section of your &lt;code&gt;Deployment.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] env: - name: DB_host value: pg-{{ .Release.Name }}.{{ .Release.Namespace }}.svc - name: DB_dbname valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-name - name: DB_user valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-user - name: DB_password valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-password [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your Helm chart is now ready to be packaged and installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package better-helm/with-templ $ helm upgrade --install ps1 person-service-templ-0.0.10.tgz &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately, one of the resulting manifest files is a &lt;code&gt;DeploymentConfig&lt;/code&gt;, which would only work on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. As a result, this chart can't be installed on any other Kubernetes distribution. So let's discuss other options.&lt;/p&gt; &lt;h2&gt;Install a Kubernetes Operator with your chart&lt;/h2&gt; &lt;p&gt;Another way to install a dependent database with your Helm chart is to look for a Kubernetes Operator on &lt;a href="https://operatorhub.io/"&gt;OperatorHub&lt;/a&gt;. If your cluster already has an Operator Lifecycle Manager (OLM) installed (as all OpenShift clusters do), then the only thing you need to do is create a &lt;code&gt;Subscription&lt;/code&gt; that describes your desire to install an Operator.&lt;/p&gt; &lt;p&gt;For example, to install the community operator by CrunchyData into OpenShift, you would need to create the following file:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: postgresql-operator namespace: openshift-operators spec: channel: v5 name: postgresql source: community-operators sourceNamespace: openshift-marketplace &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you put this file into the &lt;code&gt;crds&lt;/code&gt; folder of your Helm chart, Helm takes care of installing the Operator before it processes the template files of the chart. Please note, however, that Helm will &lt;em&gt;never&lt;/em&gt; uninstall the custom resource definitions, so the Operator will stay on the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;If you place the following file into the &lt;code&gt;templates&lt;/code&gt; folder of the chart, your PostgreSQL database instance will be ready to use:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: postgres-operator.crunchydata.com/v1beta1 kind: PostgresCluster metadata: name: {{ .Release.Name }}-db labels: app.kubernetes.io/part-of: {{ .Release.Name }}-chart spec: image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres:centos8-13.5-0 postgresVersion: 13 instances: - name: instance1 dataVolumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi backups: pgbackrest: image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.36-0 repos: - name: repo1 volume: volumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Of course, you now need to make sure that your &lt;code&gt;person-service&lt;/code&gt; is able to connect to this PostgreSQL instance. Simply add a &lt;code&gt;secretRef&lt;/code&gt; to the deployment file with the following content:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] envFrom: - secretRef: name: {{ .Release.Name }}-db-pguser-{{ .Release.Name }}-db prefix: DB_ [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will map all values of the &lt;code&gt;PostgresCluster&lt;/code&gt; secret to your deployment with a prefix of &lt;code&gt;DB_&lt;/code&gt;, which is exactly what you need.&lt;/p&gt; &lt;p&gt;Now your chart is ready to be packaged and can be installed in any OpenShift namespace:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package with-crds $ helm install ps1 person-service-crd-0.0.10.tgz $ helm uninstall ps1 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install the database by adding a subchart dependency&lt;/h2&gt; &lt;p&gt;The last option is to use a subchart within your chart. For this scenario, Helm has a &lt;a href="https://helm.sh/docs/topics/charts/#chart-dependencies"&gt;dependency management system&lt;/a&gt; that makes it easier for you as a chart developer to use third-party charts. The example that follows makes use of the &lt;a href="https://artifacthub.io/packages/helm/bitnami/postgresql"&gt;Bitnami PostgreSQL chart&lt;/a&gt;, which you can find on &lt;a href="https://artifacthub.io"&gt;ArtifactHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To start, you have to change the &lt;code&gt;Chart.yaml&lt;/code&gt; file to add the external dependency. With the following lines, you can add the dependency to the Bitnami PostgreSQL database with the version &lt;code&gt;11.1.3&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;dependencies: - name: postgresql repository: https://charts.bitnami.com/bitnami version: 11.1.3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want to define properties from within your &lt;code&gt;values.yaml&lt;/code&gt; file, you simply need to use the name of the chart as the first parameter in the tree; in this case, it is &lt;code&gt;postgresql&lt;/code&gt;. You can then add all necessary parameters below that key:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;postgresql: auth: username: wanja [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, you need to have a look at the documentation of the Bitnami chart to understand how to use it in your target environment (OpenShift, in this case). Unfortunately, as of the time of this writing, the current documentation is a bit outdated, so you would not be able to install your chart without digging further into the &lt;code&gt;values.yaml&lt;/code&gt; file Bitnami supplies to see which security settings you have to set in order to use it with OpenShift's strong enterprise security.&lt;/p&gt; &lt;p&gt;To save you the trouble, I've put together this minimum list of settings you would need to use:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;postgresql: auth: username: wanja password: wanja database: wanjadb primary: podSecurityContext: enabled: false fsGroup: "" containerSecurityContext: enabled: false runAsUser: "auto" readReplicas: podSecurityContext: enabled: false fsGroup: "" containerSecurityContext: enabled: false runAsUser: "auto" volumePermissions: enabled: false securityContext: runAsUser: "auto" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The final step is to make sure that your deployment is able to connect to this database.&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] env: - name: DB_user value: wanja - name: DB_password valueFrom: secretKeyRef: name: {{ .Release.Name }}-postgresql key: password - name: DB_dbname value: wanjadb - name: DB_host value: {{ .Release.Name }}-postgresql.{{ .Release.Namespace }}.svc [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to package your chart. Because you're depending on a third-party chart, you need to use the &lt;code&gt;-u&lt;/code&gt; option, which downloads the dependencies into the &lt;code&gt;charts&lt;/code&gt; folder of your Helm chart.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package -u better-helm/with-subchart $ helm install ps1 person-service-sub.0.0.11.tgz &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Using Helm charts for your own projects is quite easy, even if you need to make sure certain dependencies are being installed as well. Thanks to Helm's dependency management, you can easily use subcharts with your charts. And thanks to the flexibility of Helm, you can also either use a (processed) template or quickly install a Kubernetes Operator before proceeding.&lt;/p&gt; &lt;p&gt;Check out these articles to learn more about Helm:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/24/deploy-helm-charts-jenkins-cicd-red-hat-openshift-4"&gt;Deploy Helm charts with Jenkins CI/CD in Red Hat OpenShift 4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/19/deploy-java-application-using-helm-part-1"&gt;Deploy a Java application using Helm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;And for a more in-depth look at the example explored here, check out my e-book, &lt;a href="https://developers.redhat.com/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;&lt;em&gt;Getting GitOps: A Practical Platform with OpenShift, Argo CD and Tekton&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/07/3-ways-install-database-helm-charts" title="3 ways to install a database with Helm charts"&gt;3 ways to install a database with Helm charts&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Wanja Pernath</dc:creator><dc:date>2022-04-07T07:00:00Z</dc:date></entry></feed>
